{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Travely Data Lake\n",
    "## Data Engineering Capstone Project\n",
    "\n",
    "### Project Summary\n",
    "This project implements a Data Lake on S3 in parquet and json format, using mainly pandas, PySpark, and the AWS CLI. The data used includes datasets on US immigration, US demographics, worldwide daily temperatures, and airports.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running the Jupyter notebook or the Python script, please\n",
    "- make sure the Python packages required are installed - all imports are in the first cell below.\n",
    "- make sure you have the AWS CLI installed. If you are on a Linux machine (like the VM provided in the Udacity workspace), you could use the `aws_cli_linux_install.sh` script (by running the command `bash aws_cli_linux_install.sh`). You can test if your aws cli works by running `aws --version` in a terminal.\n",
    "- insert your AWS credentials, i.e. access key and secret key, in the `aws.cfg` file for the corresponding variables (without quotes around them). The IAM user you use needs to have full S3 permissions.\n",
    "- create an S3 bucket you would like to write the parquet and json files to, and insert its name as the `DEST_BUCKET` variable in the `aws.cfg` file. Example name: `s3://travely-data-lake`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Preparations: Imports, installs, configuration, path variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sh\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/9c/796934ee6d990d504c600056aa435e31bd49dbfba37e81d2045d37c8bdaf/sh-1.13.1-py2.py3-none-any.whl (40kB)\n",
      "\u001b[K    100% |████████████████████████████████| 40kB 2.9MB/s ta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: sh\n",
      "Successfully installed sh-1.13.1\n"
     ]
    }
   ],
   "source": [
    "# imports and installs here\n",
    "!pip install sh\n",
    "\n",
    "import os, time, json, sh, configparser\n",
    "from sh import aws\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read configuration file with AWS credentials\n",
    "config = configparser.ConfigParser()\n",
    "config.read('aws.cfg')\n",
    "\n",
    "# make AWS credentials accessible to the AWS CLI\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "# make bucket and folder names accessible to the AWS CLI in the shell\n",
    "# not strictly necessary for this script, but helpful if you want to \n",
    "#    execute some commands directly in the command line\n",
    "os.environ['DEST_BUCKET']=config['S3']['DEST_BUCKET']\n",
    "os.environ['IMM_KEY']=config['S3']['IMM_KEY']\n",
    "os.environ['DESC_KEY']=config['S3']['DESC_KEY']\n",
    "os.environ['TEMPERATURE_KEY']=config['S3']['TEMPERATURE_KEY']\n",
    "os.environ['DEMOGRAPHICS_KEY']=config['S3']['DEMOGRAPHICS_KEY']\n",
    "os.environ['AIRPORT_KEY']=config['S3']['AIRPORT_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define the paths where the raw data is\n",
    "imm_folder_loc = \"../../data/18-83510-I94-Data-2016\"\n",
    "airport_file_loc = \"./airport-codes_csv.csv\"\n",
    "demographics_file_loc = \"./us-cities-demographics.csv\"\n",
    "temperature_file_loc = \"../../data2/GlobalLandTemperaturesByCity.csv\"\n",
    "\n",
    "# define the destination paths where the data should go, e.g. on S3\n",
    "# the destination S3 bucket\n",
    "bucket = \"s3://udacity-dend-capstone\"\n",
    "# names of folders where data is written to\n",
    "imm_key = \"immigration/\" \n",
    "airport_key = \"airport_codes/\"\n",
    "demographics_key = \"demographics/\"\n",
    "temperature_key = \"temperature/\"\n",
    "desc_key = \"desc/\"\n",
    "# base folder for local data writing\n",
    "local = \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "\n",
    "### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc.\n",
    "\n",
    "### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Scope of the project\n",
    "Travely, a US-based touristic tour provider, would like to analyze data of people traveling to the US by plane. They would like to improve their offerings of day-tours and longer guided travels to meet their potential customers' needs. Additionally, they would like to know more about where people arrive and which months are heaviest in travelling so they can best advertise accordingly.\n",
    "\n",
    "For this, they would like to have a data lake including data on people flying in to the US, the length of their stay, the airports and the respective cities and the weather on days with many arrivals. They want it to be accessible mainly to their data science team, who are all well-versed with Spark SQL, and it should be as inexpensive as possible for the time being.\n",
    "\n",
    "For this project, I will use PySpark to process the data and AWS S3 to store it in parquet format.\n",
    "\n",
    "### Describe and Gather Data\n",
    "I am using the datasets provided by Udacity. These include:\n",
    "\n",
    "**US immigration data**: A dataset that includes flight passenger data collected at immigration, such as the airport, arrival and departure, birthyear, gender, airline, etc.\n",
    "\n",
    "**Airport codes**: A dataset about airports, including their international and local codes, country, municipality, coordinates etc.\n",
    "\n",
    "**City temperature data**: A dataset about temperature in global cities, including data from the 18th to the 21st century, such as city, country, coordinates, temperature and temperature uncertainty.\n",
    "\n",
    "**US cities demographic**: A dataset about US cities, including the state, total population, and other factors such as average household size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc. - done separately for datasets, in the ETL process. See \"Explanation\" part below.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data - done separately for datasets, in the ETL process. See \"Explanation\" part below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "For each of the datasets, the pipeline process is either:\n",
    "**SAS-format data (immigration dataset)**\n",
    "* read data into Spark DataFrame\n",
    "* explore data, identify data quality issues\n",
    "* clean data, select relevant columns, rename columns to match snakecase_convention and to ease JOINing tables together\n",
    "* write data to parquet files locally\n",
    "* copy data to S3 using the AWS CLI\n",
    "\n",
    "**CSV-format data (demographics, airport, temperature datasets)**\n",
    "* read data into pandas DataFrame\n",
    "* explore data, identify data quality issues\n",
    "* clean data, select relevant columns, rename columns to match snakecase_convention and to ease JOINing tables together\n",
    "* read the prepared data from the pandas DataFrame into a Spark DataFrame\n",
    "* write data to parquet files locally\n",
    "* copy data to S3 using the AWS CLI\n",
    "\n",
    "**Why did I choose these steps?**\n",
    "see Step 5 - rationale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model - done separately for datasets, in the ETL process. See \"Explanation\" part below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks - done separately for datasets, in the ETL process. See \"Explanation\" part below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 5: Complete Project Write Up\n",
    "#### Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "* **pandas DataFrames** offer very fast data processing in tables, especially with csv as data source, as well as integrated table-displaying abilities.\n",
    "* **PySpark** offers great Big Data processing abilities, a SQL API and a great range of options to adapt to many data formats. At scale, it also provides amazing distributed computing capabilities, which makes this solution easily scalable.\n",
    "* **AWS S3** is a very inexpensive Cloud storage service with high-availability and no big up-front costs due to its pay-only-what-you-use model. It is well integrated with other AWS services, such as EMR, which could be a good option for scaling the project up (as discussed below in the part about the three scenarios).\n",
    "* **Apache Parquet file format** is part of the Hadoop ecosystem, a very important Big Data ecosystem that Spark is part of as well. Parquet is a columnar storage which has several advantages over other formats like csv, including higher performance, only reading the minimum required amount of data, support of compression, and lesser size. This often results in faster queries and computations, less storage used, and thus fewer costs. This is ideal for data scientists using Spark to query the datasets. (Source and more details: https://databricks.com/glossary/what-is-parquet)\n",
    "* **writing locally and then copying to S3**: While doing this project, I found that writing directly to S3 was a) very slow and b) used a lot of PUT, LIST etc. requests (which are more expensive than GET requests). Thus I decided to write the (in comparison to the CSV and SAS files) small parquet files to the local Udacity workspace and then copy them to S3. The results were astonishing, as the immigration data example shows: Writing the parquet files directly to S3 had not finished even after 6 hours, while first writing them locally and then copying to S3 took only around 15 + 2 min (yet always under 20 min).\n",
    "\n",
    "For the reasons listed above I think that my technology choices are very well adapted to the problem presented. For scaling options, see the part about the three scenarios below.\n",
    "\n",
    "#### Propose how often the data should be updated and why.\n",
    "The data I used in this project does not have immediate updates. For Travely's immediate needs, no update is necessary for the time being.\n",
    "\n",
    "Depending on the data source, one might consider different options as listed below:\n",
    "* **Immigration data from the US National Tourism and Trade Office**: has different (paid!) dataset subscriptions\n",
    "* **Temperature data**: The dataset comes from https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data, which apparently is not updated anymore. The original data comes from http://berkeleyearth.org/data/, where the data seems to be updated at least yearly.\n",
    "* **Demographics data**: comes from https://public.opendatasoft.com/explore/dataset/us-cities-demographics/information/ and has not been updated since 2017. Thus, this might need to be replaced with or enriched with a more current and/or regularly updated dataset.\n",
    "* **Airport data**: According to the source (https://datahub.io/core/airport-codes#readme), this data is updated on a (near-)daily basis.\n",
    "\n",
    "Overall, if all of these sources were to be updated as often as possible when new data comes in, the airport dataset would prevail with a daily update. The best option would be to move the project to Airflow if updates should be made, and there one could schedule updating tasks according to the updating frequency of each data source separately.\n",
    "\n",
    "\n",
    "#### Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * **The data was increased by 100x**:\n",
    " In this case, I would consider moving the entire project to AWS EMR. EMR provides high-powered clusters for technologies like Hadoop and Spark, with the respective packages already installed. EMR offers integrated support for Jupyter notebooks as well as other AWS services, such as S3. You can submit a Spark job to the cluster and choose whether the cluster should keep running or terminate itself after the Spark job has finished.\n",
    " \n",
    " * **The data populates a dashboard that must be updated on a daily basis by 7am every day**:\n",
    " In this case, I would migrate the project to an Apache Airflow pipeline, which offers the option of regularly running certain steps, e.g. data ingestion, and also has the option to retry multiple times if a step fails.\n",
    " \n",
    " * **The database needed to be accessed by 100+ people**:\n",
    " In this case, I would consider different options of Data Warehouses or Database Services, such as AWS's Redshift or RDS (Relational Database Service), since probably some of these people would not be as well-versed in Spark.\n",
    " Another option would be ingesting the data into a dashboard or a BI application, depending on who these 100+ people are and in what format they can work best with the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Explanation\n",
    "Some of these steps are done for each dataset as it is processed - this is to avoid \"hopping around\" between the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Immigration Data\n",
    "\n",
    "Here we work with Spark, which has a package supporting the SAS format (.sas7bdat). Spark is a bit slow, so when I ran the cells below, each one took up to 30 s.\n",
    "\n",
    "(In this case, pandas was a lot slower, so I chose Spark, although pandas also has an option to read in SAS files.)\n",
    "\n",
    "**Please note**: For your convenience, I have commented out some of the exploring steps since they took 5-15 min each. If there was a result to be observed, I included it in a comment, along with the wall time observed with %time. - If you would like to see these steps, please feel free to de-comment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created for Immigration data\n",
      "Immigration data read into a Spark DataFrame\n"
     ]
    }
   ],
   "source": [
    "# create a Spark session with the necessary packages for the project\n",
    "spark = SparkSession.builder \\\n",
    "        .config(\"spark.jars.packages\",\\\n",
    "                \"org.apache.hadoop:hadoop-aws:2.7.1,com.amazonaws:aws-java-sdk:1.7.4,saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "print(\"SparkSession created for Immigration data\")\n",
    "\n",
    "# read one of the files into a dataframe to observe some characteristics\n",
    "df_imm =spark.read.format('com.github.saurfang.sas.spark')\\\n",
    "        .load(os.path.join(imm_folder_loc, 'i94_apr16_sub.sas7bdat'))\n",
    "print(\"Immigration data read into a Spark DataFrame\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cicid=6.0, i94yr=2016.0, i94mon=4.0, i94cit=692.0, i94res=692.0, i94port='XXX', arrdate=20573.0, i94mode=None, i94addr=None, depdate=None, i94bir=37.0, i94visa=2.0, count=1.0, dtadfile=None, visapost=None, occup=None, entdepa='T', entdepd=None, entdepu='U', matflag=None, biryear=1979.0, dtaddto='10282016', gender=None, insnum=None, airline=None, admnum=1897628485.0, fltno=None, visatype='B2'),\n",
       " Row(cicid=7.0, i94yr=2016.0, i94mon=4.0, i94cit=254.0, i94res=276.0, i94port='ATL', arrdate=20551.0, i94mode=1.0, i94addr='AL', depdate=None, i94bir=25.0, i94visa=3.0, count=1.0, dtadfile='20130811', visapost='SEO', occup=None, entdepa='G', entdepd=None, entdepu='Y', matflag=None, biryear=1991.0, dtaddto='D/S', gender='M', insnum=None, airline=None, admnum=3736796330.0, fltno='00296', visatype='F1'),\n",
       " Row(cicid=15.0, i94yr=2016.0, i94mon=4.0, i94cit=101.0, i94res=101.0, i94port='WAS', arrdate=20545.0, i94mode=1.0, i94addr='MI', depdate=20691.0, i94bir=55.0, i94visa=2.0, count=1.0, dtadfile='20160401', visapost=None, occup=None, entdepa='T', entdepd='O', entdepu=None, matflag='M', biryear=1961.0, dtaddto='09302016', gender='M', insnum=None, airline='OS', admnum=666643185.0, fltno='93', visatype='B2'),\n",
       " Row(cicid=16.0, i94yr=2016.0, i94mon=4.0, i94cit=101.0, i94res=101.0, i94port='NYC', arrdate=20545.0, i94mode=1.0, i94addr='MA', depdate=20567.0, i94bir=28.0, i94visa=2.0, count=1.0, dtadfile='20160401', visapost=None, occup=None, entdepa='O', entdepd='O', entdepu=None, matflag='M', biryear=1988.0, dtaddto='09302016', gender=None, insnum=None, airline='AA', admnum=92468461330.0, fltno='00199', visatype='B2'),\n",
       " Row(cicid=17.0, i94yr=2016.0, i94mon=4.0, i94cit=101.0, i94res=101.0, i94port='NYC', arrdate=20545.0, i94mode=1.0, i94addr='MA', depdate=20567.0, i94bir=4.0, i94visa=2.0, count=1.0, dtadfile='20160401', visapost=None, occup=None, entdepa='O', entdepd='O', entdepu=None, matflag='M', biryear=2012.0, dtaddto='09302016', gender=None, insnum=None, airline='AA', admnum=92468463130.0, fltno='00199', visatype='B2')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the first few rows to observe some characteristics\n",
    "df_imm.head(5)\n",
    "\n",
    "# observations from this and the data source's data dictionary:\n",
    "# - columns like cicid, visapost, dtadfile, dentdepa, entdepd, ... are not relevant\n",
    "#   or even completely deprecated\n",
    "# - year and month are given in intuitive numbers\n",
    "# - gender has some None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "After some observing, we would like to have all of this dataset in one Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Immigration for jan loaded\n",
      "Immigration for feb loaded\n",
      "Immigration for feb appended\n",
      "Immigration for mar loaded\n",
      "Immigration for mar appended\n",
      "Immigration for apr loaded\n",
      "Immigration for apr appended\n",
      "Immigration for may loaded\n",
      "Immigration for may appended\n",
      "Immigration for jun loaded\n",
      "Immigration for jun appended\n",
      "Immigration for jul loaded\n",
      "Immigration for jul appended\n",
      "Immigration for aug loaded\n",
      "Immigration for aug appended\n",
      "Immigration for sep loaded\n",
      "Immigration for sep appended\n",
      "Immigration for oct loaded\n",
      "Immigration for oct appended\n",
      "Immigration for nov loaded\n",
      "Immigration for nov appended\n",
      "Immigration for dec loaded\n",
      "Immigration for dec appended\n",
      "Immigration: All months loaded\n"
     ]
    }
   ],
   "source": [
    "# create dataframe by reading in january data\n",
    "df_imm =spark.read.format('com.github.saurfang.sas.spark')\\\n",
    "        .load(os.path.join(imm_folder_loc, 'i94_jan16_sub.sas7bdat'))\n",
    "print(\"Immigration for jan loaded\")\n",
    "\n",
    "# list of months\n",
    "months = [\"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]\n",
    "\n",
    "# columns the dataframe has in the end = columns of january file\n",
    "imm_columns = df_imm.columns\n",
    "\n",
    "# For some reason, the file for June has more columns, all starting with 'delete', probably deprecated.\n",
    "# I will thus select only the columns present in all of the dataframes.\n",
    "# load and append the remaining 11 months:\n",
    "for month in months:\n",
    "    df_imm_next = spark.read.format('com.github.saurfang.sas.spark')\\\n",
    "                        .load(os.path.join(imm_folder_loc, f'i94_{month}16_sub.sas7bdat'))\n",
    "    print(f\"Immigration for {month} loaded\")\n",
    "    df_imm = df_imm.union(df_imm_next[imm_columns])\n",
    "    print(f\"Immigration for {month} appended\")\n",
    "\n",
    "print(\"Immigration: All months loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Let's see how many rows we have in this dataset (result: 40,790,529 rows). There are no duplicates (number of rows stays the same when dropping duplicates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# %time df_imm.count() # wall time 8min 50s - result: 40,790,529 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_imm = df_imm.dropDuplicates()\n",
    "# %time df_imm.count() # wall time 13min 38s - result: 40,790,529 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# select only people who came by plane -> i94mode==1\n",
    "df_imm = df_imm[df_imm['i94mode']==1]\n",
    "\n",
    "# %time df_imm.count() # -> reduced to 39,166,088\n",
    "# wall time 7min 58s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_imm = df_imm.dropna(subset=['i94yr', 'i94mon', 'i94port', 'arrdate', 'i94bir']) # wall time < 1s\n",
    "\n",
    "# %time df_imm.count() # wall time 7min 53s - result: 39,1458,783 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.26 ms, sys: 64 µs, total: 8.32 ms\n",
      "Wall time: 67.9 ms\n"
     ]
    }
   ],
   "source": [
    "# select the most relevant columns for Travely:\n",
    "# year and month, airport, \n",
    "%time df_imm = df_imm['i94yr', 'i94mon', 'i94port', 'i94addr', 'i94visa', 'arrdate', 'depdate', 'biryear', 'gender', 'visatype']\n",
    "# wall time < 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(year,DoubleType,true),StructField(month,DoubleType,true),StructField(airport_code,StringType,true),StructField(address,StringType,true),StructField(visacode,DoubleType,true),StructField(arrdate,DoubleType,true),StructField(depdate,DoubleType,true),StructField(biryear,DoubleType,true),StructField(gender,StringType,true),StructField(visatype,StringType,true)))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename columns, e.g. i94yr -> year, i94port -> airport_code\n",
    "df_imm = df_imm.withColumnRenamed(\"i94yr\", \"year\") \\\n",
    "                .withColumnRenamed(\"i94mon\", \"month\") \\\n",
    "                .withColumnRenamed(\"i94port\", \"airport_code\") \\\n",
    "                .withColumnRenamed(\"i94addr\", \"address\") \\\n",
    "                .withColumnRenamed(\"i94visa\", \"visacode\")\n",
    "df_imm.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(year,DoubleType,true),StructField(month,DoubleType,true),StructField(airport_code,StringType,true),StructField(address,StringType,true),StructField(visacode,DoubleType,true),StructField(biryear,DoubleType,true),StructField(gender,StringType,true),StructField(visatype,StringType,true),StructField(stay_duration,DoubleType,true)))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert dates and add+compute duration column\n",
    "date_converter = F.udf(lambda x: datetime.fromordinal(x), T.DateType())\n",
    "df_imm = df_imm.withColumn(\"stay_duration\", (F.col(\"depdate\") - F.col(\"arrdate\")))\n",
    "df_imm = df_imm.drop(\"depdate\").drop(\"arrdate\")\n",
    "df_imm.schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# df_imm.head(10) # controlling that stay_duration is correctly implemented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 125 ms, sys: 12.3 ms, total: 138 ms\n",
      "Wall time: 15min 11s\n"
     ]
    }
   ],
   "source": [
    "# write data to parquet files\n",
    "\n",
    "# Here I chose to write the parquet files locally and then use the aws cli to upload them to the S3 bucket.\n",
    "# Time comparison: 15-20 min writing + 1-2 min uploading \n",
    "# vs. writing directly to S3: not finished even after 6 hours\n",
    "\n",
    "%time df_imm.write \\\n",
    "            .partitionBy('month').mode('overwrite') \\\n",
    "            .parquet(os.path.join(local, imm_key))\n",
    "\n",
    "# Here, I only partitioned by month, since all the data is from the year 2016 \n",
    "# and it would thus not make sense to include the year in the partitioning process.\n",
    "# However, should Travely decide to include more immigration from other years, \n",
    "# year would definitely be the first partitioning key, followed by month.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The I94 immigration data also had some annotations in the file I94_SAS_Labels_Descriptions.SAS. From this, I have created the respective JSON files (such as address_desc.json, in the desc folder), which will generate more tables for the final data model. (This is not included as code since I had to manually fix the files at some points.) Joining the immigration data and these tables will give more information about the encoded information, such as the state/country the immigrating person comes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define function for regular S3 upload where nothing needs to be excluded,\n",
    "# and variables that simplify it\n",
    "onlyerrors = \"--only-show-errors\"\n",
    "s3 = \"s3\"\n",
    "sync = \"sync\"\n",
    "\n",
    "def aws_upload(key, bucket):\n",
    "    \"\"\"Uploads files from a local folder with name key to a folder of the same name in an S3 Bucket named bucket.\"\"\"\n",
    "    aws(s3, sync, os.path.join(local, key), os.path.join(bucket, key), onlyerrors, _fg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Immigration description data: json files uploaded to S3\n"
     ]
    }
   ],
   "source": [
    "# upload description json files to S3\n",
    "# command in bash: aws s3 sync ./desc/ s3://udacity-dend-capstone/desc/ --exclude \"*ipynb*\" --only-show-errors\n",
    "# this uses the sh module and the `from sh import aws` import \n",
    "\n",
    "aws(s3, sync, os.path.join(local, desc_key), os.path.join(bucket, desc_key), '--exclude', '*ipynb*', onlyerrors, _fg=True)\n",
    "print(\"Immigration description data: json files uploaded to S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Immigration data: parquet files uploaded to S3\n"
     ]
    }
   ],
   "source": [
    "# upload immigration parquet files to S3\n",
    "# command in bash: aws s3 sync ./imm_data/ s3://udacity-dend-capstone/immigration/ --only-show-errors\n",
    "aws_upload(imm_key, bucket)\n",
    "print(\"Immigration data: parquet files uploaded to S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Immigration data: PROCESSING FINISHED\n"
     ]
    }
   ],
   "source": [
    "print(\"Immigration data: PROCESSING FINISHED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Temperature Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Since the raw data is in CSV format, I will use pandas for the exploration and cleaning because it is faster and provides an easy overview through its automatic table-formatting. Each step with pandas should take under 10 s.\n",
    "\n",
    "To write the data to parquet files in S3, I will use Pyspark - unlike pandas, it does not require additional dependencies for that task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.6 s, sys: 1.26 s, total: 9.86 s\n",
      "Wall time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "# read temperature into pandas DataFrame\n",
    "%time pd_temperature = pd.read_csv(temperature_file_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8599212, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1744-04-01</td>\n",
       "      <td>5.788</td>\n",
       "      <td>3.624</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1744-05-01</td>\n",
       "      <td>10.644</td>\n",
       "      <td>1.283</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1744-06-01</td>\n",
       "      <td>14.051</td>\n",
       "      <td>1.347</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1744-07-01</td>\n",
       "      <td>16.082</td>\n",
       "      <td>1.396</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1744-08-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "5  1744-04-01               5.788                          3.624  Århus   \n",
       "6  1744-05-01              10.644                          1.283  Århus   \n",
       "7  1744-06-01              14.051                          1.347  Århus   \n",
       "8  1744-07-01              16.082                          1.396  Århus   \n",
       "9  1744-08-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  \n",
       "5  Denmark   57.05N    10.33E  \n",
       "6  Denmark   57.05N    10.33E  \n",
       "7  Denmark   57.05N    10.33E  \n",
       "8  Denmark   57.05N    10.33E  \n",
       "9  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get some information about the data\n",
    "print(pd_temperature.shape)\n",
    "pd_temperature.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here we can already see there are some NaN values in the dataset. In the next step, we will determine how many values in which column are NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt                                    0\n",
       "AverageTemperature               364130\n",
       "AverageTemperatureUncertainty    364130\n",
       "City                                  0\n",
       "Country                               0\n",
       "Latitude                              0\n",
       "Longitude                             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine the number of nulls/NaNs in the data\n",
    "pd_temperature.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As we can see, only AverageTemperature and AverageTemperatureUncertainty have NaN values, and they have the same number of NaN values.\n",
    "Thus, we assume they are always either both NaN or both have a non-null value (as seen above in the first few lines of data).\n",
    "\n",
    "In the next step, we will drop the NaN values from the dataframe and verify there are no more NaN values in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt                               0\n",
      "AverageTemperature               0\n",
      "AverageTemperatureUncertainty    0\n",
      "City                             0\n",
      "Country                          0\n",
      "Latitude                         0\n",
      "Longitude                        0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1744-04-01</td>\n",
       "      <td>5.788</td>\n",
       "      <td>3.624</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1744-05-01</td>\n",
       "      <td>10.644</td>\n",
       "      <td>1.283</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1744-06-01</td>\n",
       "      <td>14.051</td>\n",
       "      <td>1.347</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1744-07-01</td>\n",
       "      <td>16.082</td>\n",
       "      <td>1.396</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1744-09-01</td>\n",
       "      <td>12.781</td>\n",
       "      <td>1.454</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1744-10-01</td>\n",
       "      <td>7.950</td>\n",
       "      <td>1.630</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1744-11-01</td>\n",
       "      <td>4.639</td>\n",
       "      <td>1.302</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1744-12-01</td>\n",
       "      <td>0.122</td>\n",
       "      <td>1.756</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1745-01-01</td>\n",
       "      <td>-1.333</td>\n",
       "      <td>1.642</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0   1743-11-01               6.068                          1.737  Århus   \n",
       "5   1744-04-01               5.788                          3.624  Århus   \n",
       "6   1744-05-01              10.644                          1.283  Århus   \n",
       "7   1744-06-01              14.051                          1.347  Århus   \n",
       "8   1744-07-01              16.082                          1.396  Århus   \n",
       "10  1744-09-01              12.781                          1.454  Århus   \n",
       "11  1744-10-01               7.950                          1.630  Århus   \n",
       "12  1744-11-01               4.639                          1.302  Århus   \n",
       "13  1744-12-01               0.122                          1.756  Århus   \n",
       "14  1745-01-01              -1.333                          1.642  Århus   \n",
       "\n",
       "    Country Latitude Longitude  \n",
       "0   Denmark   57.05N    10.33E  \n",
       "5   Denmark   57.05N    10.33E  \n",
       "6   Denmark   57.05N    10.33E  \n",
       "7   Denmark   57.05N    10.33E  \n",
       "8   Denmark   57.05N    10.33E  \n",
       "10  Denmark   57.05N    10.33E  \n",
       "11  Denmark   57.05N    10.33E  \n",
       "12  Denmark   57.05N    10.33E  \n",
       "13  Denmark   57.05N    10.33E  \n",
       "14  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop NaN values\n",
    "pd_temperature = pd_temperature.dropna()\n",
    "\n",
    "# check that NaN values have been dropped\n",
    "print(pd_temperature.isnull().sum())\n",
    "pd_temperature.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Since Travely currently only operates in the US, we will select the temperature values for Country='United States', and also just include data after 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42404, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49715</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>8.039</td>\n",
       "      <td>0.180</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49716</th>\n",
       "      <td>2000-02-01</td>\n",
       "      <td>11.908</td>\n",
       "      <td>0.306</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49717</th>\n",
       "      <td>2000-03-01</td>\n",
       "      <td>14.423</td>\n",
       "      <td>0.385</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49718</th>\n",
       "      <td>2000-04-01</td>\n",
       "      <td>18.274</td>\n",
       "      <td>0.262</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49719</th>\n",
       "      <td>2000-05-01</td>\n",
       "      <td>25.358</td>\n",
       "      <td>0.358</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49720</th>\n",
       "      <td>2000-06-01</td>\n",
       "      <td>25.264</td>\n",
       "      <td>0.412</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49721</th>\n",
       "      <td>2000-07-01</td>\n",
       "      <td>29.421</td>\n",
       "      <td>0.345</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49722</th>\n",
       "      <td>2000-08-01</td>\n",
       "      <td>29.733</td>\n",
       "      <td>0.354</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49723</th>\n",
       "      <td>2000-09-01</td>\n",
       "      <td>25.446</td>\n",
       "      <td>0.305</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49724</th>\n",
       "      <td>2000-10-01</td>\n",
       "      <td>18.477</td>\n",
       "      <td>0.262</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               dt  AverageTemperature  AverageTemperatureUncertainty     City  \\\n",
       "49715  2000-01-01               8.039                          0.180  Abilene   \n",
       "49716  2000-02-01              11.908                          0.306  Abilene   \n",
       "49717  2000-03-01              14.423                          0.385  Abilene   \n",
       "49718  2000-04-01              18.274                          0.262  Abilene   \n",
       "49719  2000-05-01              25.358                          0.358  Abilene   \n",
       "49720  2000-06-01              25.264                          0.412  Abilene   \n",
       "49721  2000-07-01              29.421                          0.345  Abilene   \n",
       "49722  2000-08-01              29.733                          0.354  Abilene   \n",
       "49723  2000-09-01              25.446                          0.305  Abilene   \n",
       "49724  2000-10-01              18.477                          0.262  Abilene   \n",
       "\n",
       "             Country Latitude Longitude  \n",
       "49715  United States   32.95N   100.53W  \n",
       "49716  United States   32.95N   100.53W  \n",
       "49717  United States   32.95N   100.53W  \n",
       "49718  United States   32.95N   100.53W  \n",
       "49719  United States   32.95N   100.53W  \n",
       "49720  United States   32.95N   100.53W  \n",
       "49721  United States   32.95N   100.53W  \n",
       "49722  United States   32.95N   100.53W  \n",
       "49723  United States   32.95N   100.53W  \n",
       "49724  United States   32.95N   100.53W  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select only US\n",
    "pd_temperature = pd_temperature[pd_temperature['Country']=='United States']\n",
    "# select only timestamps after 2000-01-01 (including)\n",
    "pd_temperature = pd_temperature[pd_temperature['dt']>='2000-01-01']\n",
    "\n",
    "# get information about dataframe\n",
    "print(pd_temperature.shape)\n",
    "pd_temperature.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Before writing the table to file, I will rename the columns so they are a bit shorter (in the case of the temperature columns) and match the usual snake-case convention (e.g. AverageTemperature -> avg_temperature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>avg_temperature</th>\n",
       "      <th>avg_temperature_uncert</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49715</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>8.039</td>\n",
       "      <td>0.180</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49716</th>\n",
       "      <td>2000-02-01</td>\n",
       "      <td>11.908</td>\n",
       "      <td>0.306</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49717</th>\n",
       "      <td>2000-03-01</td>\n",
       "      <td>14.423</td>\n",
       "      <td>0.385</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49718</th>\n",
       "      <td>2000-04-01</td>\n",
       "      <td>18.274</td>\n",
       "      <td>0.262</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49719</th>\n",
       "      <td>2000-05-01</td>\n",
       "      <td>25.358</td>\n",
       "      <td>0.358</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49720</th>\n",
       "      <td>2000-06-01</td>\n",
       "      <td>25.264</td>\n",
       "      <td>0.412</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49721</th>\n",
       "      <td>2000-07-01</td>\n",
       "      <td>29.421</td>\n",
       "      <td>0.345</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49722</th>\n",
       "      <td>2000-08-01</td>\n",
       "      <td>29.733</td>\n",
       "      <td>0.354</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49723</th>\n",
       "      <td>2000-09-01</td>\n",
       "      <td>25.446</td>\n",
       "      <td>0.305</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49724</th>\n",
       "      <td>2000-10-01</td>\n",
       "      <td>18.477</td>\n",
       "      <td>0.262</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               dt  avg_temperature  avg_temperature_uncert     city  \\\n",
       "49715  2000-01-01            8.039                   0.180  Abilene   \n",
       "49716  2000-02-01           11.908                   0.306  Abilene   \n",
       "49717  2000-03-01           14.423                   0.385  Abilene   \n",
       "49718  2000-04-01           18.274                   0.262  Abilene   \n",
       "49719  2000-05-01           25.358                   0.358  Abilene   \n",
       "49720  2000-06-01           25.264                   0.412  Abilene   \n",
       "49721  2000-07-01           29.421                   0.345  Abilene   \n",
       "49722  2000-08-01           29.733                   0.354  Abilene   \n",
       "49723  2000-09-01           25.446                   0.305  Abilene   \n",
       "49724  2000-10-01           18.477                   0.262  Abilene   \n",
       "\n",
       "             country latitude longitude  \n",
       "49715  United States   32.95N   100.53W  \n",
       "49716  United States   32.95N   100.53W  \n",
       "49717  United States   32.95N   100.53W  \n",
       "49718  United States   32.95N   100.53W  \n",
       "49719  United States   32.95N   100.53W  \n",
       "49720  United States   32.95N   100.53W  \n",
       "49721  United States   32.95N   100.53W  \n",
       "49722  United States   32.95N   100.53W  \n",
       "49723  United States   32.95N   100.53W  \n",
       "49724  United States   32.95N   100.53W  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename columns\n",
    "pd_temperature.rename(columns = {\"AverageTemperature\": \"avg_temperature\",\n",
    "                                \"AverageTemperatureUncertainty\": \"avg_temperature_uncert\",\n",
    "                                \"City\": \"city\",\n",
    "                                \"Country\": \"country\",\n",
    "                                \"Latitude\": \"latitude\",\n",
    "                                \"Longitude\": \"longitude\"},\n",
    "                     inplace = True)\n",
    "\n",
    "# verify successful rename by looking at the header of the table\n",
    "pd_temperature.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Finally, we write the data into parquet files using Spark - after a small data quality check. The transformation of the pandas DataFrame to the Spark DataFrame, the file writing, and the S3 upload should each only take a few seconds.\n",
    "\n",
    "To avoid high charges for S3 LIST etc. operations and to make the process faster, I will first write the files to the workspace and then copy them to the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create a SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\",\\\n",
    "                \"org.apache.hadoop:hadoop-aws:2.7.1,com.amazonaws:aws-java-sdk:1.7.4,saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# read data from pandas DataFrame into Spark DataFrame\n",
    "df_temperature = spark.createDataFrame(pd_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Check: data frame not empty, passed\n",
      "Data Quality Check: multiple cities, passed\n",
      "Data Quality Check: multiple and dates, passed\n"
     ]
    }
   ],
   "source": [
    "# check that dataframe is not empty\n",
    "if df_temperature.head(1) != 0:\n",
    "    print(\"Data Quality Check: data frame not empty, passed\")\n",
    "else:\n",
    "    print(\"DATAFRAME EMPTY\")\n",
    "\n",
    "# check that there are multiple cities and dates\n",
    "if df_temperature.groupby(\"city\").count().head(1) != 0 :\n",
    "    print(\"Data Quality Check: multiple cities, passed\")\n",
    "else:\n",
    "    print(\"Data Quality Check FAILED: missing cities\")\n",
    "\n",
    "if df_temperature.groupby(\"dt\").count().head(1) != 0:\n",
    "    print(\"Data Quality Check: multiple and dates, passed\")\n",
    "else:\n",
    "    print(\"Data Quality Check FAILED: missing or dates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.68 ms, sys: 325 µs, total: 3 ms\n",
      "Wall time: 4.37 s\n"
     ]
    }
   ],
   "source": [
    "# write data to parquet\n",
    "%time df_temperature.write \\\n",
    "                .partitionBy('city') \\\n",
    "                .mode('overwrite') \\\n",
    "                .parquet(os.path.join(local, temperature_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature data: uploaded to S3\n"
     ]
    }
   ],
   "source": [
    "# upload files to S3 using the AWS CLI\n",
    "# bash version: !aws s3 sync ./temperature/ s3://udacity-dend-capstone/temperature/ --only-show-errors\n",
    "aws_upload(temperature_key, bucket)\n",
    "print(\"Temperature data: uploaded to S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature data: PROCESSING FINISHED\n"
     ]
    }
   ],
   "source": [
    "print(\"Temperature data: PROCESSING FINISHED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Airport Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00AS</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Fulton Airport</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-OK</td>\n",
       "      <td>Alex</td>\n",
       "      <td>00AS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AS</td>\n",
       "      <td>-97.8180194, 34.9428028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00AZ</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Cordes Airport</td>\n",
       "      <td>3810.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AZ</td>\n",
       "      <td>Cordes</td>\n",
       "      <td>00AZ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AZ</td>\n",
       "      <td>-112.16500091552734, 34.305599212646484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00CA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Goldstone /Gts/ Airport</td>\n",
       "      <td>3038.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-CA</td>\n",
       "      <td>Barstow</td>\n",
       "      <td>00CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00CA</td>\n",
       "      <td>-116.888000488, 35.350498199499995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00CL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Williams Ag Airport</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-CA</td>\n",
       "      <td>Biggs</td>\n",
       "      <td>00CL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00CL</td>\n",
       "      <td>-121.763427, 39.427188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00CN</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Kitchen Creek Helibase Heliport</td>\n",
       "      <td>3350.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-CA</td>\n",
       "      <td>Pine Valley</td>\n",
       "      <td>00CN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00CN</td>\n",
       "      <td>-116.4597417, 32.7273736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "5  00AS  small_airport                      Fulton Airport        1100.0   \n",
       "6  00AZ  small_airport                      Cordes Airport        3810.0   \n",
       "7  00CA  small_airport             Goldstone /Gts/ Airport        3038.0   \n",
       "8  00CL  small_airport                 Williams Ag Airport          87.0   \n",
       "9  00CN       heliport     Kitchen Creek Helibase Heliport        3350.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "5       NaN          US      US-OK          Alex     00AS       NaN   \n",
       "6       NaN          US      US-AZ        Cordes     00AZ       NaN   \n",
       "7       NaN          US      US-CA       Barstow     00CA       NaN   \n",
       "8       NaN          US      US-CA         Biggs     00CL       NaN   \n",
       "9       NaN          US      US-CA   Pine Valley     00CN       NaN   \n",
       "\n",
       "  local_code                              coordinates  \n",
       "0        00A       -74.93360137939453, 40.07080078125  \n",
       "1       00AA                   -101.473911, 38.704022  \n",
       "2       00AK              -151.695999146, 59.94919968  \n",
       "3       00AL    -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                      -91.254898, 35.6087  \n",
       "5       00AS                  -97.8180194, 34.9428028  \n",
       "6       00AZ  -112.16500091552734, 34.305599212646484  \n",
       "7       00CA       -116.888000488, 35.350498199499995  \n",
       "8       00CL                   -121.763427, 39.427188  \n",
       "9       00CN                 -116.4597417, 32.7273736  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data into pandas DataFrame\n",
    "pd_airport = pd.read_csv(airport_file_loc, delimiter=\",\")\n",
    "\n",
    "# display a few lines of data to get to know it\n",
    "pd_airport.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55075, 12)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine shape of the data, i.e. # columns and rows\n",
    "pd_airport.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55075 entries, 0 to 55074\n",
      "Data columns (total 12 columns):\n",
      "ident           55075 non-null object\n",
      "type            55075 non-null object\n",
      "name            55075 non-null object\n",
      "elevation_ft    48069 non-null float64\n",
      "continent       27356 non-null object\n",
      "iso_country     54828 non-null object\n",
      "iso_region      55075 non-null object\n",
      "municipality    49399 non-null object\n",
      "gps_code        41030 non-null object\n",
      "iata_code       9189 non-null object\n",
      "local_code      28686 non-null object\n",
      "coordinates     55075 non-null object\n",
      "dtypes: float64(1), object(11)\n",
      "memory usage: 5.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# get information about data\n",
    "pd_airport.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Since in the immigration data only airports with an international airport code are given (also known as IATA code), we will remove any rows without an IATA code from the airport table. Then, we will check how many rows are left in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9189, 12)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9189 entries, 223 to 55070\n",
      "Data columns (total 12 columns):\n",
      "ident           9189 non-null object\n",
      "type            9189 non-null object\n",
      "name            9189 non-null object\n",
      "elevation_ft    8819 non-null float64\n",
      "continent       6222 non-null object\n",
      "iso_country     9158 non-null object\n",
      "iso_region      9189 non-null object\n",
      "municipality    8423 non-null object\n",
      "gps_code        8538 non-null object\n",
      "iata_code       9189 non-null object\n",
      "local_code      2987 non-null object\n",
      "coordinates     9189 non-null object\n",
      "dtypes: float64(1), object(11)\n",
      "memory usage: 933.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# drop all rows without an IATA code\n",
    "pd_airport = pd_airport.dropna(subset=[\"iata_code\"])\n",
    "\n",
    "# see how the DataFrame has changed in size\n",
    "print(pd_airport.shape)\n",
    "pd_airport.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "For joining with the immigration table, columns like ident, local_code, gps_code, and continent are not as relevant, thus we will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>small_airport</td>\n",
       "      <td>Utirik Airport</td>\n",
       "      <td>4.0</td>\n",
       "      <td>MH</td>\n",
       "      <td>MH-UTI</td>\n",
       "      <td>Utirik Island</td>\n",
       "      <td>UTK</td>\n",
       "      <td>169.852005, 11.222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>small_airport</td>\n",
       "      <td>Ocean Reef Club Airport</td>\n",
       "      <td>8.0</td>\n",
       "      <td>US</td>\n",
       "      <td>US-FL</td>\n",
       "      <td>Key Largo</td>\n",
       "      <td>OCA</td>\n",
       "      <td>-80.274803161621, 25.325399398804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>small_airport</td>\n",
       "      <td>Pilot Station Airport</td>\n",
       "      <td>305.0</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Pilot Station</td>\n",
       "      <td>PQS</td>\n",
       "      <td>-162.899994, 61.934601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>small_airport</td>\n",
       "      <td>Crested Butte Airpark</td>\n",
       "      <td>8980.0</td>\n",
       "      <td>US</td>\n",
       "      <td>US-CO</td>\n",
       "      <td>Crested Butte</td>\n",
       "      <td>CSE</td>\n",
       "      <td>-106.928341, 38.851918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>small_airport</td>\n",
       "      <td>LBJ Ranch Airport</td>\n",
       "      <td>1515.0</td>\n",
       "      <td>US</td>\n",
       "      <td>US-TX</td>\n",
       "      <td>Johnson City</td>\n",
       "      <td>JCY</td>\n",
       "      <td>-98.62249755859999, 30.251800537100003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>small_airport</td>\n",
       "      <td>Metropolitan Airport</td>\n",
       "      <td>418.0</td>\n",
       "      <td>US</td>\n",
       "      <td>US-MA</td>\n",
       "      <td>Palmer</td>\n",
       "      <td>PMX</td>\n",
       "      <td>-72.31140136719999, 42.223300933800004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>seaplane_base</td>\n",
       "      <td>Loring Seaplane Base</td>\n",
       "      <td>0.0</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Loring</td>\n",
       "      <td>WLR</td>\n",
       "      <td>-131.636993408, 55.6012992859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1555</th>\n",
       "      <td>small_airport</td>\n",
       "      <td>Nunapitchuk Airport</td>\n",
       "      <td>12.0</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Nunapitchuk</td>\n",
       "      <td>NUP</td>\n",
       "      <td>-162.440454, 60.905591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>seaplane_base</td>\n",
       "      <td>Port Alice Seaplane Base</td>\n",
       "      <td>0.0</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Port Alice</td>\n",
       "      <td>PTC</td>\n",
       "      <td>-133.597, 55.803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>small_airport</td>\n",
       "      <td>Icy Bay Airport</td>\n",
       "      <td>50.0</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Icy Bay</td>\n",
       "      <td>ICY</td>\n",
       "      <td>-141.662002563, 59.96900177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               type                      name  elevation_ft iso_country  \\\n",
       "223   small_airport            Utirik Airport           4.0          MH   \n",
       "440   small_airport   Ocean Reef Club Airport           8.0          US   \n",
       "594   small_airport     Pilot Station Airport         305.0          US   \n",
       "673   small_airport     Crested Butte Airpark        8980.0          US   \n",
       "1088  small_airport         LBJ Ranch Airport        1515.0          US   \n",
       "1402  small_airport      Metropolitan Airport         418.0          US   \n",
       "1438  seaplane_base      Loring Seaplane Base           0.0          US   \n",
       "1555  small_airport       Nunapitchuk Airport          12.0          US   \n",
       "1574  seaplane_base  Port Alice Seaplane Base           0.0          US   \n",
       "1722  small_airport           Icy Bay Airport          50.0          US   \n",
       "\n",
       "     iso_region   municipality iata_code  \\\n",
       "223      MH-UTI  Utirik Island       UTK   \n",
       "440       US-FL      Key Largo       OCA   \n",
       "594       US-AK  Pilot Station       PQS   \n",
       "673       US-CO  Crested Butte       CSE   \n",
       "1088      US-TX   Johnson City       JCY   \n",
       "1402      US-MA         Palmer       PMX   \n",
       "1438      US-AK         Loring       WLR   \n",
       "1555      US-AK    Nunapitchuk       NUP   \n",
       "1574      US-AK     Port Alice       PTC   \n",
       "1722      US-AK        Icy Bay       ICY   \n",
       "\n",
       "                                 coordinates  \n",
       "223                       169.852005, 11.222  \n",
       "440        -80.274803161621, 25.325399398804  \n",
       "594                   -162.899994, 61.934601  \n",
       "673                   -106.928341, 38.851918  \n",
       "1088  -98.62249755859999, 30.251800537100003  \n",
       "1402  -72.31140136719999, 42.223300933800004  \n",
       "1438           -131.636993408, 55.6012992859  \n",
       "1555                  -162.440454, 60.905591  \n",
       "1574                        -133.597, 55.803  \n",
       "1722             -141.662002563, 59.96900177  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop less relevant columns and verify\n",
    "pd_airport.drop(columns = [\"ident\", \"local_code\", \"continent\", \"gps_code\"], inplace=True)\n",
    "pd_airport.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "To make joins more intuitive, we will rename some of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airport_type</th>\n",
       "      <th>airport_name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>airport_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>small_airport</td>\n",
       "      <td>Utirik Airport</td>\n",
       "      <td>4.0</td>\n",
       "      <td>MH</td>\n",
       "      <td>MH-UTI</td>\n",
       "      <td>Utirik Island</td>\n",
       "      <td>UTK</td>\n",
       "      <td>169.852005, 11.222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>small_airport</td>\n",
       "      <td>Ocean Reef Club Airport</td>\n",
       "      <td>8.0</td>\n",
       "      <td>US</td>\n",
       "      <td>US-FL</td>\n",
       "      <td>Key Largo</td>\n",
       "      <td>OCA</td>\n",
       "      <td>-80.274803161621, 25.325399398804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>small_airport</td>\n",
       "      <td>Pilot Station Airport</td>\n",
       "      <td>305.0</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Pilot Station</td>\n",
       "      <td>PQS</td>\n",
       "      <td>-162.899994, 61.934601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>small_airport</td>\n",
       "      <td>Crested Butte Airpark</td>\n",
       "      <td>8980.0</td>\n",
       "      <td>US</td>\n",
       "      <td>US-CO</td>\n",
       "      <td>Crested Butte</td>\n",
       "      <td>CSE</td>\n",
       "      <td>-106.928341, 38.851918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>small_airport</td>\n",
       "      <td>LBJ Ranch Airport</td>\n",
       "      <td>1515.0</td>\n",
       "      <td>US</td>\n",
       "      <td>US-TX</td>\n",
       "      <td>Johnson City</td>\n",
       "      <td>JCY</td>\n",
       "      <td>-98.62249755859999, 30.251800537100003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       airport_type             airport_name  elevation_ft iso_country  \\\n",
       "223   small_airport           Utirik Airport           4.0          MH   \n",
       "440   small_airport  Ocean Reef Club Airport           8.0          US   \n",
       "594   small_airport    Pilot Station Airport         305.0          US   \n",
       "673   small_airport    Crested Butte Airpark        8980.0          US   \n",
       "1088  small_airport        LBJ Ranch Airport        1515.0          US   \n",
       "\n",
       "     iso_region   municipality airport_code  \\\n",
       "223      MH-UTI  Utirik Island          UTK   \n",
       "440       US-FL      Key Largo          OCA   \n",
       "594       US-AK  Pilot Station          PQS   \n",
       "673       US-CO  Crested Butte          CSE   \n",
       "1088      US-TX   Johnson City          JCY   \n",
       "\n",
       "                                 coordinates  \n",
       "223                       169.852005, 11.222  \n",
       "440        -80.274803161621, 25.325399398804  \n",
       "594                   -162.899994, 61.934601  \n",
       "673                   -106.928341, 38.851918  \n",
       "1088  -98.62249755859999, 30.251800537100003  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename columns\n",
    "pd_airport.rename(columns = {\"type\": \"airport_type\",\n",
    "                            \"name\": \"airport_name\",\n",
    "                            \"iata_code\": \"airport_code\"},\n",
    "                 inplace = True)\n",
    "pd_airport.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Check - passed: No missing airport names or airport codes\n"
     ]
    }
   ],
   "source": [
    "# data quality check: relevant columns do not contain null/NaN\n",
    "if (pd_airport.isna().sum()[\"airport_name\"] == 0) and (pd_airport.isna().sum()[\"airport_code\"] == 0):\n",
    "    print(\"Data Quality Check - passed: No missing airport names or airport codes\")\n",
    "else:\n",
    "    print(\"Data Quality Check - FAILED: Missing airport names or airport codes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The last step is to write this data to files and copy these to S3, using Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\",\\\n",
    "                \"org.apache.hadoop:hadoop-aws:2.7.1,com.amazonaws:aws-java-sdk:1.7.4,saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# create schema for data\n",
    "schema = T.StructType([T.StructField(\"airport_type\", T.StringType()),\n",
    "                      T.StructField(\"airport_name\", T.StringType()),\n",
    "                      T.StructField(\"elevation_ft\", T.DoubleType()),\n",
    "                      T.StructField(\"iso_country\", T.StringType()),\n",
    "                      T.StructField(\"iso_region\", T.StringType()),\n",
    "                      T.StructField(\"municipality\", T.StringType()),\n",
    "                      T.StructField(\"airport_code\", T.StringType()),\n",
    "                      T.StructField(\"coordinates\", T.StringType())])\n",
    "\n",
    "# read data from pandas DataFrame into Spark DataFrame\n",
    "df_airport = spark.createDataFrame(pd_airport, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.66 ms, sys: 0 ns, total: 2.66 ms\n",
      "Wall time: 2.64 s\n"
     ]
    }
   ],
   "source": [
    "# write data to parquet\n",
    "%time df_airport.write \\\n",
    "                .partitionBy('iso_country') \\\n",
    "                .mode('overwrite') \\\n",
    "                .parquet(os.path.join(local, airport_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airport data: uploaded to S3\n",
      "Airport data: PROCESSING FINISHED\n"
     ]
    }
   ],
   "source": [
    "# upload files to S3 using the AWS CLI\n",
    "# bash version: !aws s3 sync ./airport_codes/ s3://udacity-dend-capstone/airport_codes/ --only-show-errors\n",
    "aws_upload(airport_key, bucket)\n",
    "print(\"Airport data: uploaded to S3\")\n",
    "print(\"Airport data: PROCESSING FINISHED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Demographics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Peoria</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>33.1</td>\n",
       "      <td>56229.0</td>\n",
       "      <td>62432.0</td>\n",
       "      <td>118661</td>\n",
       "      <td>6634.0</td>\n",
       "      <td>7517.0</td>\n",
       "      <td>2.40</td>\n",
       "      <td>IL</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>1343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Avondale</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>29.1</td>\n",
       "      <td>38712.0</td>\n",
       "      <td>41971.0</td>\n",
       "      <td>80683</td>\n",
       "      <td>4815.0</td>\n",
       "      <td>8355.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>11592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>West Covina</td>\n",
       "      <td>California</td>\n",
       "      <td>39.8</td>\n",
       "      <td>51629.0</td>\n",
       "      <td>56860.0</td>\n",
       "      <td>108489</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>37038.0</td>\n",
       "      <td>3.56</td>\n",
       "      <td>CA</td>\n",
       "      <td>Asian</td>\n",
       "      <td>32716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>O'Fallon</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>36.0</td>\n",
       "      <td>41762.0</td>\n",
       "      <td>43270.0</td>\n",
       "      <td>85032</td>\n",
       "      <td>5783.0</td>\n",
       "      <td>3269.0</td>\n",
       "      <td>2.77</td>\n",
       "      <td>MO</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>2583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>High Point</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>35.5</td>\n",
       "      <td>51751.0</td>\n",
       "      <td>58077.0</td>\n",
       "      <td>109828</td>\n",
       "      <td>5204.0</td>\n",
       "      <td>16315.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>NC</td>\n",
       "      <td>Asian</td>\n",
       "      <td>11060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City           State  Median Age  Male Population  \\\n",
       "0     Silver Spring        Maryland        33.8          40601.0   \n",
       "1            Quincy   Massachusetts        41.0          44129.0   \n",
       "2            Hoover         Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga      California        34.5          88127.0   \n",
       "4            Newark      New Jersey        34.6         138040.0   \n",
       "5            Peoria        Illinois        33.1          56229.0   \n",
       "6          Avondale         Arizona        29.1          38712.0   \n",
       "7       West Covina      California        39.8          51629.0   \n",
       "8          O'Fallon        Missouri        36.0          41762.0   \n",
       "9        High Point  North Carolina        35.5          51751.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "5            62432.0            118661              6634.0        7517.0   \n",
       "6            41971.0             80683              4815.0        8355.0   \n",
       "7            56860.0            108489              3800.0       37038.0   \n",
       "8            43270.0             85032              5783.0        3269.0   \n",
       "9            58077.0            109828              5204.0       16315.0   \n",
       "\n",
       "   Average Household Size State Code                               Race  Count  \n",
       "0                    2.60         MD                 Hispanic or Latino  25924  \n",
       "1                    2.39         MA                              White  58723  \n",
       "2                    2.58         AL                              Asian   4759  \n",
       "3                    3.18         CA          Black or African-American  24437  \n",
       "4                    2.73         NJ                              White  76402  \n",
       "5                    2.40         IL  American Indian and Alaska Native   1343  \n",
       "6                    3.18         AZ          Black or African-American  11592  \n",
       "7                    3.56         CA                              Asian  32716  \n",
       "8                    2.77         MO                 Hispanic or Latino   2583  \n",
       "9                    2.65         NC                              Asian  11060  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data into pandas DataFrame\n",
    "pd_demographics = pd.read_csv(demographics_file_loc, delimiter=\";\")\n",
    "\n",
    "# look at a few rows to get to know the data\n",
    "pd_demographics.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2891, 12)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine the size of the DataFrame\n",
    "pd_demographics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City                       0\n",
      "State                      0\n",
      "Median Age                 0\n",
      "Male Population            3\n",
      "Female Population          3\n",
      "Total Population           0\n",
      "Number of Veterans        13\n",
      "Foreign-born              13\n",
      "Average Household Size    16\n",
      "State Code                 0\n",
      "Race                       0\n",
      "Count                      0\n",
      "dtype: int64\n",
      "City                      0\n",
      "State                     0\n",
      "Median Age                0\n",
      "Male Population           0\n",
      "Female Population         0\n",
      "Total Population          0\n",
      "Number of Veterans        0\n",
      "Foreign-born              0\n",
      "Average Household Size    0\n",
      "State Code                0\n",
      "Race                      0\n",
      "Count                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# look at the number of null/NaN values\n",
    "print(pd_demographics.isna().sum())\n",
    "\n",
    "# drop NaN values\n",
    "pd_demographics.dropna(inplace = True)\n",
    "\n",
    "# verify\n",
    "print(pd_demographics.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Race is not relevant in any way for Travely, so the colums Race and Count can be omitted. The information in the other columns is the same for any \"Race\", so no aggregations are necessary here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(588, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code  \n",
       "0                    2.60         MD  \n",
       "1                    2.39         MA  \n",
       "2                    2.58         AL  \n",
       "3                    3.18         CA  \n",
       "4                    2.73         NJ  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop irrelevant columns\n",
    "pd_demographics.drop(columns = [\"Race\", \"Count\"], inplace = True)\n",
    "\n",
    "# since now the rest of the rows is each a duplicate (or even a \"quadruplicate\"),\n",
    "# drop the repeated rows\n",
    "pd_demographics.drop_duplicates(inplace = True)\n",
    "\n",
    "# verify and look at size of DataFrame\n",
    "print(pd_demographics.shape)\n",
    "pd_demographics.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>median_age</th>\n",
       "      <th>male_population</th>\n",
       "      <th>female_population</th>\n",
       "      <th>population</th>\n",
       "      <th>veterans</th>\n",
       "      <th>foreign_born</th>\n",
       "      <th>avg_household_size</th>\n",
       "      <th>state_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               city          state  median_age  male_population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   female_population  population  veterans  foreign_born  avg_household_size  \\\n",
       "0            41862.0       82463    1562.0       30908.0                2.60   \n",
       "1            49500.0       93629    4147.0       32935.0                2.39   \n",
       "2            46799.0       84839    4819.0        8229.0                2.58   \n",
       "3            87105.0      175232    5821.0       33878.0                3.18   \n",
       "4           143873.0      281913    5829.0       86253.0                2.73   \n",
       "\n",
       "  state_code  \n",
       "0         MD  \n",
       "1         MA  \n",
       "2         AL  \n",
       "3         CA  \n",
       "4         NJ  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename columns for easier joining\n",
    "# and to match snake_case\n",
    "pd_demographics.rename(columns = {\"City\": \"city\",\n",
    "                                  \"State\": \"state\",\n",
    "                                  \"Median Age\": \"median_age\",\n",
    "                                  \"Male Population\": \"male_population\",\n",
    "                                  \"Female Population\": \"female_population\",\n",
    "                                  \"Total Population\": \"population\",\n",
    "                                  \"Number of Veterans\": \"veterans\",\n",
    "                                  \"Foreign-born\": \"foreign_born\",\n",
    "                                  \"Average Household Size\": \"avg_household_size\",\n",
    "                                  \"State Code\": \"state_code\"},\n",
    "                      inplace = True)\n",
    "pd_demographics.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "city                   object\n",
       "state                  object\n",
       "median_age            float64\n",
       "male_population       float64\n",
       "female_population     float64\n",
       "population              int64\n",
       "veterans              float64\n",
       "foreign_born          float64\n",
       "avg_household_size    float64\n",
       "state_code             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check datatypes of columns\n",
    "pd_demographics.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# people occur only in integers -> change type to int64 for columns male_population, female_population, veterans, foreign_born\n",
    "pd_demographics = pd_demographics.astype({\"male_population\": \"int64\",\n",
    "                                          \"female_population\": \"int64\",\n",
    "                                          \"veterans\": \"int64\",\n",
    "                                          \"foreign_born\": \"int64\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "city                   object\n",
       "state                  object\n",
       "median_age            float64\n",
       "male_population         int64\n",
       "female_population       int64\n",
       "population              int64\n",
       "veterans                int64\n",
       "foreign_born            int64\n",
       "avg_household_size    float64\n",
       "state_code             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify\n",
    "pd_demographics.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Data Quality Checks\n",
    "\n",
    "We would like the following conditions to be true:\n",
    "\n",
    "* population = male_population + female_population\n",
    "* population > foreign_born\n",
    "* population > avg_household_size\n",
    "* male_population > veterans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality issues in population count: 0\n",
      "Data quality issues in foreign_born: 0\n",
      "Data quality issues in avg_household_size: 0\n",
      "Data quality issues in veterans: 0\n"
     ]
    }
   ],
   "source": [
    "# errors = 0\n",
    "\n",
    "# for index, row in pd_demographics.iterrows():\n",
    "#     if not (row.population >= (row.male_population + row.female_population)):\n",
    "#         # since some people might self-identify as neither male or female, there could be more, but not less people than males+females\n",
    "#         errors += 1\n",
    "# print(f\"Data quality issues in population count: {errors}\")\n",
    "\n",
    "# errors = 0\n",
    "# for index, row in pd_demographics.iterrows():\n",
    "#     if row.population < row.foreign_born:\n",
    "#         errors += 1\n",
    "# print(f\"Data quality issues in foreign_born: {errors}\")\n",
    "# errors = 0\n",
    "\n",
    "# for index, row in pd_demographics.iterrows():\n",
    "#     if row.population < row.avg_household_size:\n",
    "#         errors += 1\n",
    "# print(f\"Data quality issues in avg_household_size: {errors}\")\n",
    "# errors = 0\n",
    "\n",
    "# for index, row in pd_demographics.iterrows():\n",
    "#     if row.male_population < row.veterans:\n",
    "#         errors += 1\n",
    "# print(f\"Data quality issues in veterans: {errors}\")\n",
    "\n",
    "def data_quality_check(condition_wanted, message):\n",
    "    errors = 0\n",
    "    for index, row in pd_demographics.iterrows():\n",
    "        if not eval(condition_wanted):\n",
    "            errors += 1\n",
    "    print(message)\n",
    "\n",
    "data_quality_check(\"row.population >= (row.male_population + row.female_population)\",\n",
    "                   f\"Data quality issues in population count: {errors}\")\n",
    "data_quality_check(\"row.population > row.foreign_born\",\n",
    "                  f\"Data quality issues in foreign_born: {errors}\")\n",
    "data_quality_check(\"row.population > row.avg_household_size\",\n",
    "                  f\"Data quality issues in avg_household_size: {errors}\")\n",
    "data_quality_check(\"row.male_population > row.veterans\",\n",
    "                  f\"Data quality issues in veterans: {errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\",\\\n",
    "                \"org.apache.hadoop:hadoop-aws:2.7.1,com.amazonaws:aws-java-sdk:1.7.4,saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.47 ms, sys: 181 µs, total: 1.65 ms\n",
      "Wall time: 1.14 s\n"
     ]
    }
   ],
   "source": [
    "# read data from pandas DataFrame into Spark DataFrame\n",
    "df_demographics = spark.createDataFrame(pd_demographics)\n",
    "\n",
    "# write data to parquet locally\n",
    "%time df_demographics.write \\\n",
    "                .partitionBy('state_code') \\\n",
    "                .mode('overwrite') \\\n",
    "                .parquet(os.path.join(local, demographics_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographics data: uploaded to S3\n"
     ]
    }
   ],
   "source": [
    "# upload files to S3 using the AWS CLI\n",
    "# bash version: !aws s3 sync ./demographics/ s3://udacity-dend-capstone/demographics/ --only-show-errors\n",
    "aws_upload(demographics_key, bucket)\n",
    "print(\"Demographics data: uploaded to S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographics data: PROCESSING FINISHED\n"
     ]
    }
   ],
   "source": [
    "print(\"Demographics data: PROCESSING FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
