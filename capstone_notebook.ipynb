{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Travely Data Lake\n",
    "## Data Engineering Capstone Project\n",
    "\n",
    "### Project Summary\n",
    "This project implements a Data Lake on S3 in parquet and json format, using mainly pandas, PySpark, and the AWS CLI. The data used includes datasets on US immigration, US demographics, worldwide daily temperatures, and airports.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running the Jupyter notebook or the Python script, please\n",
    "- make sure the Python packages required are installed - all imports are in the first cell below.\n",
    "- make sure you have the AWS CLI installed. If you are on a Linux machine (like the VM provided in the Udacity workspace), you could use the `aws_cli_linux_install.sh` script (by running the command `bash aws_cli_linux_install.sh`). You can test if your aws cli works by running `aws --version` in a terminal.\n",
    "- insert your AWS credentials, i.e. access key and secret key, in the `aws.cfg` file for the corresponding variables (without quotes around them). The IAM user you use needs to have full S3 permissions.\n",
    "- create an S3 bucket you would like to write the parquet and json files to, and insert its name as the `DEST_BUCKET` variable in the `aws.cfg` file. Example name: `s3://travely-data-lake`\n",
    "- if you want to, you can change the folder names the files will be written to, both locally and on S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Preparations: Imports, installs, configuration, path variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sh in /opt/conda/lib/python3.6/site-packages (1.13.1)\n"
     ]
    }
   ],
   "source": [
    "# imports and installs\n",
    "!pip install sh\n",
    "\n",
    "import os, time, json, sh, configparser\n",
    "from sh import aws\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read configuration file with AWS credentials\n",
    "config = configparser.ConfigParser()\n",
    "config.read('aws.cfg')\n",
    "\n",
    "# make AWS credentials accessible to the AWS CLI\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "# make bucket and folder names accessible to the AWS CLI in the shell\n",
    "# not strictly necessary for this script, but helpful if you want to \n",
    "#    execute some commands directly in the command line\n",
    "os.environ['DEST_BUCKET']=config['S3']['DEST_BUCKET']\n",
    "os.environ['IMM_KEY']=config['S3']['IMM_KEY']\n",
    "os.environ['DESC_KEY']=config['S3']['DESC_KEY']\n",
    "os.environ['TEMPERATURE_KEY']=config['S3']['TEMPERATURE_KEY']\n",
    "os.environ['DEMOGRAPHICS_KEY']=config['S3']['DEMOGRAPHICS_KEY']\n",
    "os.environ['AIRPORT_KEY']=config['S3']['AIRPORT_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define the paths where the raw data is\n",
    "# imm_folder_loc = \"../../data/18-83510-I94-Data-2016\"\n",
    "# airport_file_loc = \"./airport-codes_csv.csv\"\n",
    "# demographics_file_loc = \"./us-cities-demographics.csv\"\n",
    "# temperature_file_loc = \"../../data2/GlobalLandTemperaturesByCity.csv\"\n",
    "imm_folder_loc = config['LOCAL']['IMM_FOLDER_LOC']\n",
    "airport_file_loc = config['LOCAL']['AIRPORT_FILE_LOC']\n",
    "demographics_file_loc = config['LOCAL']['DEMOGRAPHICS_FILE_LOC']\n",
    "temperature_file_loc = config['LOCAL']['TEMPERATURE_FILE_LOC']\n",
    "\n",
    "# base folder for local data writing\n",
    "local = config['LOCAL']['LOCAL_KEY']\n",
    "\n",
    "# define the destination paths where the data should go, e.g. on S3\n",
    "# the destination S3 bucket\n",
    "bucket = config['S3']['DEST_BUCKET']\n",
    "# names of folders where data is written to\n",
    "imm_key = config['S3']['IMM_KEY']\n",
    "airport_key = config['S3']['AIRPORT_KEY']\n",
    "demographics_key = config['S3']['DEMOGRAPHICS_KEY']\n",
    "temperature_key = config['S3']['TEMPERATURE_KEY']\n",
    "desc_key = config['S3']['DESC_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "### Scope of the project\n",
    "* Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc.\n",
    "\n",
    "Travely, a US-based touristic tour provider, would like to analyze data of people traveling to the US by plane. They would like to improve their offerings of day-tours and longer guided travels to meet their potential customers' needs. Additionally, they would like to know more about where people arrive and which months are heaviest in travelling so they can best advertise accordingly.\n",
    "\n",
    "For this, they would like to have a data lake including data on people flying in to the US, the length of their stay, the airports and the respective cities and the weather on days with many arrivals. They want it to be accessible mainly to their data science team, who are all well-versed with Spark SQL, and it should be as inexpensive as possible for the time being.\n",
    "\n",
    "For this project, I will use PySpark to process the data and AWS S3 to store it in parquet format.\n",
    "\n",
    "### Describe and Gather Data\n",
    "* Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "\n",
    "I am using the datasets provided by Udacity. These include:\n",
    "\n",
    "**US immigration data**: A dataset that includes flight passenger data collected at immigration, such as the airport, arrival and departure, birthyear, gender, airline, etc.\n",
    "\n",
    "**Airport codes**: A dataset about airports, including their international and local codes, country, municipality, coordinates etc.\n",
    "\n",
    "**City temperature data**: A dataset about temperature in global cities, including data from the 18th to the 21st century, such as city, country, coordinates, temperature and temperature uncertainty.\n",
    "\n",
    "**US cities demographic**: A dataset about US cities, including the state, total population, and other factors such as average household size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc. - done separately for datasets, in the ETL process. See \"Explanation\" part below.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data - done separately for datasets, in the ETL process. See \"Explanation\" part below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "I chose a snowflake model with a fact-table (immigration) and several dimension tables (demographics, airport_codes, temperature, several desc tables) that can be joined onto it and some even onto each other. Possible JOINs are indicated with -> other_table.column_name.\n",
    "\n",
    "immigration:\n",
    "- year\n",
    "- month\n",
    "- airport_code\n",
    "- address\n",
    "- visacode\n",
    "- biryear\n",
    "- gender\n",
    "- visatype\n",
    "- stay_duration\n",
    "\n",
    "temperature:\n",
    "- dt\n",
    "- avg_temperature\n",
    "- avg_temperature_uncert\n",
    "- city -> airport_codes.municipality, demographics.city\n",
    "- country\n",
    "- latitude\n",
    "- longitude\n",
    "\n",
    "airport_codes\n",
    "- airport_type\n",
    "- airport_name\n",
    "- elevation_ft\n",
    "- iso_country\n",
    "- iso_region\n",
    "- municipality -> demographics.city, temperature.city\n",
    "- airport_code -> immigration.airport_code\n",
    "- coordinates\n",
    "\n",
    "demographics:\n",
    "- city -> airport_codes.municipality, temperature.city\n",
    "- state\n",
    "- median_age\n",
    "- male_population\n",
    "- female_population\n",
    "- population\n",
    "- veterans\n",
    "- foreign_born\n",
    "- avg_household_size\n",
    "- state_code\n",
    "\n",
    "address_desc:\n",
    "- key -> immigration.address\n",
    "- value\n",
    "\n",
    "airport_desc:\n",
    "- key -> immigration.airport_code\n",
    "- value\n",
    "\n",
    "visacode_desc:\n",
    "- key -> immigration.visacode\n",
    "- value\n",
    "\n",
    "A data dictionary can be found in `datadict.md`.\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "For each of the datasets, the pipeline process is either:\n",
    "**SAS-format data (immigration dataset)**\n",
    "* read data into Spark DataFrame\n",
    "* explore data, identify data quality issues\n",
    "* clean data, select relevant columns, rename columns to match snakecase_convention and to ease JOINing tables together\n",
    "* write data to parquet files locally\n",
    "* copy data to S3 using the AWS CLI\n",
    "\n",
    "**CSV-format data (demographics, airport, temperature datasets)**\n",
    "* read data into pandas DataFrame\n",
    "* explore data, identify data quality issues\n",
    "* clean data, select relevant columns, rename columns to match snakecase_convention and to ease JOINing tables together\n",
    "* read the prepared data from the pandas DataFrame into a Spark DataFrame\n",
    "* write data to parquet files locally\n",
    "* copy data to S3 using the AWS CLI\n",
    "\n",
    "**Why did I choose these steps?**\n",
    "see Step 5 - rationale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model - done separately for datasets, in the ETL process. See \"Explanation\" part below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks - done separately for datasets, in the ETL process. See \"Explanation\" part below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 5: Complete Project Write Up\n",
    "#### Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "* **pandas DataFrames** offer very fast data processing in tables, especially with csv as data source, as well as integrated table-displaying abilities.\n",
    "* **PySpark** offers great Big Data processing abilities, a SQL API and a great range of options to adapt to many data formats. At scale, it also provides amazing distributed computing capabilities, which makes this solution easily scalable.\n",
    "* **AWS S3** is a very inexpensive Cloud storage service with high-availability and no big up-front costs due to its pay-only-what-you-use model. It is well integrated with other AWS services, such as EMR, which could be a good option for scaling the project up (as discussed below in the part about the three scenarios).\n",
    "* **Apache Parquet file format** is part of the Hadoop ecosystem, a very important Big Data ecosystem that Spark is part of as well. Parquet is a columnar storage which has several advantages over other formats like csv, including higher performance, only reading the minimum required amount of data, support of compression, and lesser size. This often results in faster queries and computations, less storage used, and thus fewer costs. This is ideal for data scientists using Spark to query the datasets. (Source and more details: https://databricks.com/glossary/what-is-parquet)\n",
    "* **writing locally and then copying to S3**: While doing this project, I found that writing directly to S3 was a) very slow and b) used a lot of PUT, LIST etc. requests (which are more expensive than GET requests). Thus I decided to write the (in comparison to the CSV and SAS files) small parquet files to the local Udacity workspace and then copy them to S3. The results were astonishing, as the immigration data example shows: Writing the parquet files directly to S3 had not finished even after 6 hours, while first writing them locally and then copying to S3 took only around 15 + 2 min (yet always under 20 min).\n",
    "\n",
    "For the reasons listed above I think that my technology choices are very well adapted to the problem presented. For scaling options, see the part about the three scenarios below.\n",
    "\n",
    "#### Propose how often the data should be updated and why.\n",
    "The data I used in this project does not have immediate updates. For Travely's immediate needs, no update is necessary for the time being.\n",
    "\n",
    "Depending on the data source, one might consider different options as listed below:\n",
    "* **Immigration data from the US National Tourism and Trade Office**: has different (paid!) dataset subscriptions\n",
    "* **Temperature data**: The dataset comes from https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data, which apparently is not updated anymore. The original data comes from http://berkeleyearth.org/data/, where the data seems to be updated at least yearly.\n",
    "* **Demographics data**: comes from https://public.opendatasoft.com/explore/dataset/us-cities-demographics/information/ and has not been updated since 2017. Thus, this might need to be replaced with or enriched with a more current and/or regularly updated dataset.\n",
    "* **Airport data**: According to the source (https://datahub.io/core/airport-codes#readme), this data is updated on a (near-)daily basis.\n",
    "\n",
    "Overall, if all of these sources were to be updated as often as possible when new data comes in, the airport dataset would prevail with a daily update. The best option would be to move the project to Airflow if updates should be made, and there one could schedule updating tasks according to the updating frequency of each data source separately.\n",
    "\n",
    "\n",
    "#### Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * **The data was increased by 100x**:\n",
    " In this case, I would consider moving the entire project to AWS EMR. EMR provides high-powered clusters for technologies like Hadoop and Spark, with the respective packages already installed. EMR offers integrated support for Jupyter notebooks as well as other AWS services, such as S3. You can submit a Spark job to the cluster and choose whether the cluster should keep running or terminate itself after the Spark job has finished.\n",
    " \n",
    " * **The data populates a dashboard that must be updated on a daily basis by 7am every day**:\n",
    " In this case, I would migrate the project to an Apache Airflow pipeline, which offers the option of regularly running certain steps, e.g. data ingestion, and also has the option to retry multiple times if a step fails.\n",
    " \n",
    " * **The database needed to be accessed by 100+ people**:\n",
    " In this case, I would consider different options of Data Warehouses or Database Services, such as AWS's Redshift or RDS (Relational Database Service), since probably some of these people would not be as well-versed in Spark.\n",
    " Another option would be ingesting the data into a dashboard or a BI application, depending on who these 100+ people are and in what format they can work best with the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Explanation\n",
    "Some of these steps are done for each dataset as it is processed - this is to avoid \"hopping around\" between the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Immigration Data\n",
    "\n",
    "Here we work with Spark, which has a package supporting the SAS format (.sas7bdat). Spark is a bit slow, so when I ran the cells below, each one took up to 30 s.\n",
    "\n",
    "(In this case, pandas was a lot slower, so I chose Spark, although pandas also has an option to read in SAS files.)\n",
    "\n",
    "**Please note**: For your convenience, I have commented out some of the exploring steps since they took 5-15 min each. If there was a result to be observed, I included it in a comment, along with the wall time observed with %time. - If you would like to see these steps, please feel free to de-comment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created for Immigration data\n",
      "Immigration data read into a Spark DataFrame\n"
     ]
    }
   ],
   "source": [
    "# create a Spark session with the necessary packages for the project\n",
    "spark = SparkSession.builder \\\n",
    "        .config(\"spark.jars.packages\",\\\n",
    "                \"org.apache.hadoop:hadoop-aws:2.7.1,com.amazonaws:aws-java-sdk:1.7.4,saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "print(\"SparkSession created for Immigration data\")\n",
    "\n",
    "# read one of the files into a dataframe to observe some characteristics\n",
    "df_imm =spark.read.format('com.github.saurfang.sas.spark')\\\n",
    "        .load(os.path.join(imm_folder_loc, 'i94_apr16_sub.sas7bdat'))\n",
    "print(\"Immigration data read into a Spark DataFrame\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cicid=6.0, i94yr=2016.0, i94mon=4.0, i94cit=692.0, i94res=692.0, i94port='XXX', arrdate=20573.0, i94mode=None, i94addr=None, depdate=None, i94bir=37.0, i94visa=2.0, count=1.0, dtadfile=None, visapost=None, occup=None, entdepa='T', entdepd=None, entdepu='U', matflag=None, biryear=1979.0, dtaddto='10282016', gender=None, insnum=None, airline=None, admnum=1897628485.0, fltno=None, visatype='B2'),\n",
       " Row(cicid=7.0, i94yr=2016.0, i94mon=4.0, i94cit=254.0, i94res=276.0, i94port='ATL', arrdate=20551.0, i94mode=1.0, i94addr='AL', depdate=None, i94bir=25.0, i94visa=3.0, count=1.0, dtadfile='20130811', visapost='SEO', occup=None, entdepa='G', entdepd=None, entdepu='Y', matflag=None, biryear=1991.0, dtaddto='D/S', gender='M', insnum=None, airline=None, admnum=3736796330.0, fltno='00296', visatype='F1'),\n",
       " Row(cicid=15.0, i94yr=2016.0, i94mon=4.0, i94cit=101.0, i94res=101.0, i94port='WAS', arrdate=20545.0, i94mode=1.0, i94addr='MI', depdate=20691.0, i94bir=55.0, i94visa=2.0, count=1.0, dtadfile='20160401', visapost=None, occup=None, entdepa='T', entdepd='O', entdepu=None, matflag='M', biryear=1961.0, dtaddto='09302016', gender='M', insnum=None, airline='OS', admnum=666643185.0, fltno='93', visatype='B2'),\n",
       " Row(cicid=16.0, i94yr=2016.0, i94mon=4.0, i94cit=101.0, i94res=101.0, i94port='NYC', arrdate=20545.0, i94mode=1.0, i94addr='MA', depdate=20567.0, i94bir=28.0, i94visa=2.0, count=1.0, dtadfile='20160401', visapost=None, occup=None, entdepa='O', entdepd='O', entdepu=None, matflag='M', biryear=1988.0, dtaddto='09302016', gender=None, insnum=None, airline='AA', admnum=92468461330.0, fltno='00199', visatype='B2'),\n",
       " Row(cicid=17.0, i94yr=2016.0, i94mon=4.0, i94cit=101.0, i94res=101.0, i94port='NYC', arrdate=20545.0, i94mode=1.0, i94addr='MA', depdate=20567.0, i94bir=4.0, i94visa=2.0, count=1.0, dtadfile='20160401', visapost=None, occup=None, entdepa='O', entdepd='O', entdepu=None, matflag='M', biryear=2012.0, dtaddto='09302016', gender=None, insnum=None, airline='AA', admnum=92468463130.0, fltno='00199', visatype='B2')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the first few rows to observe some characteristics\n",
    "df_imm.head(5)\n",
    "\n",
    "# observations from this and the data source's data dictionary:\n",
    "# - columns like cicid, visapost, dtadfile, dentdepa, entdepd, ... are not relevant\n",
    "#   or even completely deprecated\n",
    "# - year and month are given in intuitive numbers\n",
    "# - gender has some None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "After some observing, we would like to have all of this dataset in one Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Immigration for jan loaded\n",
      "Immigration for feb loaded\n",
      "Immigration for feb appended\n",
      "Immigration for mar loaded\n",
      "Immigration for mar appended\n",
      "Immigration for apr loaded\n",
      "Immigration for apr appended\n",
      "Immigration for may loaded\n",
      "Immigration for may appended\n",
      "Immigration for jun loaded\n",
      "Immigration for jun appended\n",
      "Immigration for jul loaded\n",
      "Immigration for jul appended\n",
      "Immigration for aug loaded\n",
      "Immigration for aug appended\n",
      "Immigration for sep loaded\n",
      "Immigration for sep appended\n",
      "Immigration for oct loaded\n",
      "Immigration for oct appended\n",
      "Immigration for nov loaded\n",
      "Immigration for nov appended\n",
      "Immigration for dec loaded\n",
      "Immigration for dec appended\n",
      "Immigration: All months loaded\n"
     ]
    }
   ],
   "source": [
    "# create dataframe by reading in january data\n",
    "df_imm =spark.read.format('com.github.saurfang.sas.spark')\\\n",
    "        .load(os.path.join(imm_folder_loc, 'i94_jan16_sub.sas7bdat'))\n",
    "print(\"Immigration for jan loaded\")\n",
    "\n",
    "# list of months\n",
    "months = [\"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]\n",
    "\n",
    "# columns the dataframe has in the end = columns of january file\n",
    "imm_columns = df_imm.columns\n",
    "\n",
    "# For some reason, the file for June has more columns, all starting with 'delete', probably deprecated.\n",
    "# I will thus select only the columns present in all of the dataframes.\n",
    "# load and append the remaining 11 months:\n",
    "for month in months:\n",
    "    df_imm_next = spark.read.format('com.github.saurfang.sas.spark')\\\n",
    "                        .load(os.path.join(imm_folder_loc, f'i94_{month}16_sub.sas7bdat'))\n",
    "    print(f\"Immigration for {month} loaded\")\n",
    "    df_imm = df_imm.union(df_imm_next[imm_columns])\n",
    "    print(f\"Immigration for {month} appended\")\n",
    "\n",
    "print(\"Immigration: All months loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Let's see how many rows we have in this dataset (result: 40,790,529 rows). There are no duplicates (number of rows stays the same when dropping duplicates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# %time df_imm.count() # wall time 8min 50s - result: 40,790,529 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_imm = df_imm.dropDuplicates()\n",
    "# %time df_imm.count() # wall time 13min 38s - result: 40,790,529 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# select only people who came by plane -> i94mode==1\n",
    "df_imm = df_imm[df_imm['i94mode']==1]\n",
    "\n",
    "# %time df_imm.count() # -> reduced to 39,166,088\n",
    "# wall time 7min 58s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_imm = df_imm.dropna(subset=['i94yr', 'i94mon', 'i94port', 'arrdate', 'i94bir']) # wall time < 1s\n",
    "\n",
    "# %time df_imm.count() # wall time 7min 53s - result: 39,1458,783 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.77 ms, sys: 336 µs, total: 6.1 ms\n",
      "Wall time: 63.1 ms\n"
     ]
    }
   ],
   "source": [
    "# select the most relevant columns for Travely\n",
    "%time df_imm = df_imm['i94yr', 'i94mon', 'i94port', 'i94addr', 'i94visa', 'arrdate', 'depdate', 'biryear', 'gender', 'visatype']\n",
    "# wall time < 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(year,DoubleType,true),StructField(month,DoubleType,true),StructField(airport_code,StringType,true),StructField(address,StringType,true),StructField(visacode,DoubleType,true),StructField(arrdate,DoubleType,true),StructField(depdate,DoubleType,true),StructField(biryear,DoubleType,true),StructField(gender,StringType,true),StructField(visatype,StringType,true)))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename columns, e.g. i94yr -> year, i94port -> airport_code\n",
    "df_imm = df_imm.withColumnRenamed(\"i94yr\", \"year\") \\\n",
    "                .withColumnRenamed(\"i94mon\", \"month\") \\\n",
    "                .withColumnRenamed(\"i94port\", \"airport_code\") \\\n",
    "                .withColumnRenamed(\"i94addr\", \"address\") \\\n",
    "                .withColumnRenamed(\"i94visa\", \"visacode\")\n",
    "df_imm.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(year,DoubleType,true),StructField(month,DoubleType,true),StructField(airport_code,StringType,true),StructField(address,StringType,true),StructField(visacode,DoubleType,true),StructField(biryear,DoubleType,true),StructField(gender,StringType,true),StructField(visatype,StringType,true),StructField(stay_duration,DoubleType,true)))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace date columns with duration of stay column\n",
    "date_converter = F.udf(lambda x: datetime.fromordinal(x), T.DateType())\n",
    "df_imm = df_imm.withColumn(\"stay_duration\", (F.col(\"depdate\") - F.col(\"arrdate\")))\n",
    "df_imm = df_imm.drop(\"depdate\").drop(\"arrdate\")\n",
    "df_imm.schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# df_imm.head(10) # controlling that stay_duration is correctly implemented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write data to parquet files\n",
    "\n",
    "# Here I chose to write the parquet files locally and then use the aws cli to upload them to the S3 bucket.\n",
    "# Time comparison: 15-20 min writing + 1-2 min uploading \n",
    "# vs. writing directly to S3: not finished even after 6 hours\n",
    "\n",
    "%time df_imm.write \\\n",
    "            .partitionBy('month').mode('overwrite') \\\n",
    "            .parquet(os.path.join(local, imm_key))\n",
    "\n",
    "# Here, I only partitioned by month, since all the data is from the year 2016 \n",
    "# and it would thus not make sense to include the year in the partitioning process.\n",
    "# However, should Travely decide to include more immigration from other years, \n",
    "# year would definitely be the first partitioning key, followed by month.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The I94 immigration data also had some annotations in the file I94_SAS_Labels_Descriptions.SAS. From this, I have created the respective JSON files (such as address_desc.json, in the desc folder), which will generate more tables for the final data model. (This is not included as code since I had to manually fix the files at some points.) Joining the immigration data and these tables will give more information about the encoded information, such as the state/country the immigrating person comes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define function for regular S3 upload where nothing needs to be excluded,\n",
    "# and variables that simplify it\n",
    "onlyerrors = \"--only-show-errors\"\n",
    "s3 = \"s3\"\n",
    "sync = \"sync\"\n",
    "\n",
    "def aws_upload(key, bucket):\n",
    "    \"\"\"Uploads files from a local folder with name key to a folder of the same name in an S3 Bucket named bucket.\"\"\"\n",
    "    aws(s3, sync, os.path.join(local, key), os.path.join(bucket, key), onlyerrors, _fg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# upload description json files to S3\n",
    "# command in bash: aws s3 sync ./desc/ s3://udacity-dend-capstone/desc/ --exclude \"*ipynb*\" --only-show-errors\n",
    "# this uses the sh module and the `from sh import aws` import \n",
    "\n",
    "aws(s3, sync, os.path.join(local, desc_key), os.path.join(bucket, desc_key), '--exclude', '*ipynb*', onlyerrors, _fg=True)\n",
    "print(\"Immigration description data: json files uploaded to S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# upload immigration parquet files to S3\n",
    "# command in bash: aws s3 sync ./imm_data/ s3://udacity-dend-capstone/immigration/ --only-show-errors\n",
    "aws_upload(imm_key, bucket)\n",
    "print(\"Immigration data: parquet files uploaded to S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Immigration data: PROCESSING FINISHED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Temperature Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Since the raw data is in CSV format, I will use pandas for the exploration and cleaning because it is faster and provides an easy overview through its automatic table-formatting. Each step with pandas should take under 10 s.\n",
    "\n",
    "To write the data to parquet files in S3, I will use Pyspark - unlike pandas, it does not require additional dependencies for that task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read temperature into pandas DataFrame\n",
    "%time pd_temperature = pd.read_csv(temperature_file_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get some information about the data\n",
    "print(pd_temperature.shape)\n",
    "pd_temperature.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here we can already see there are some NaN values in the dataset. In the next step, we will determine how many values in which column are NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# determine the number of nulls/NaNs in the data\n",
    "pd_temperature.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As we can see, only AverageTemperature and AverageTemperatureUncertainty have NaN values, and they have the same number of NaN values.\n",
    "Thus, we assume they are always either both NaN or both have a non-null value (as seen above in the first few lines of data).\n",
    "\n",
    "In the next step, we will drop the NaN values from the dataframe and verify there are no more NaN values in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# drop NaN values\n",
    "pd_temperature = pd_temperature.dropna()\n",
    "\n",
    "# check that NaN values have been dropped\n",
    "print(pd_temperature.isnull().sum())\n",
    "pd_temperature.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Since Travely currently only operates in the US, we will select the temperature values for Country='United States', and also just include data after 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# select only US\n",
    "pd_temperature = pd_temperature[pd_temperature['Country']=='United States']\n",
    "# select only timestamps after 2000-01-01 (including)\n",
    "pd_temperature = pd_temperature[pd_temperature['dt']>='2000-01-01']\n",
    "\n",
    "# get information about dataframe\n",
    "print(pd_temperature.shape)\n",
    "pd_temperature.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Before writing the table to file, I will rename the columns so they are a bit shorter (in the case of the temperature columns) and match the usual snake-case convention (e.g. AverageTemperature -> avg_temperature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# rename columns\n",
    "pd_temperature.rename(columns = {\"AverageTemperature\": \"avg_temperature\",\n",
    "                                \"AverageTemperatureUncertainty\": \"avg_temperature_uncert\",\n",
    "                                \"City\": \"city\",\n",
    "                                \"Country\": \"country\",\n",
    "                                \"Latitude\": \"latitude\",\n",
    "                                \"Longitude\": \"longitude\"},\n",
    "                     inplace = True)\n",
    "\n",
    "# verify successful rename by looking at the header of the table\n",
    "pd_temperature.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Finally, we write the data into parquet files using Spark - after a small data quality check. The transformation of the pandas DataFrame to the Spark DataFrame, the file writing, and the S3 upload should each only take a few seconds.\n",
    "\n",
    "To avoid high charges for S3 LIST etc. operations and to make the process faster, I will first write the files to the workspace and then copy them to the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create a SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\",\\\n",
    "                \"org.apache.hadoop:hadoop-aws:2.7.1,com.amazonaws:aws-java-sdk:1.7.4,saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# read data from pandas DataFrame into Spark DataFrame\n",
    "df_temperature = spark.createDataFrame(pd_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check that dataframe is not empty\n",
    "if df_temperature.head(1) != 0:\n",
    "    print(\"Data Quality Check: data frame not empty, passed\")\n",
    "else:\n",
    "    print(\"DATAFRAME EMPTY\")\n",
    "\n",
    "# check that there are multiple cities and dates\n",
    "if df_temperature.groupby(\"city\").count().head(1) != 0 :\n",
    "    print(\"Data Quality Check: multiple cities, passed\")\n",
    "else:\n",
    "    print(\"Data Quality Check FAILED: missing cities\")\n",
    "\n",
    "if df_temperature.groupby(\"dt\").count().head(1) != 0:\n",
    "    print(\"Data Quality Check: multiple and dates, passed\")\n",
    "else:\n",
    "    print(\"Data Quality Check FAILED: missing or dates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write data to parquet\n",
    "%time df_temperature.write \\\n",
    "                .partitionBy('city') \\\n",
    "                .mode('overwrite') \\\n",
    "                .parquet(os.path.join(local, temperature_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# upload files to S3 using the AWS CLI\n",
    "# bash version: !aws s3 sync ./temperature/ s3://udacity-dend-capstone/temperature/ --only-show-errors\n",
    "aws_upload(temperature_key, bucket)\n",
    "print(\"Temperature data: uploaded to S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Temperature data: PROCESSING FINISHED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Airport Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read data into pandas DataFrame\n",
    "pd_airport = pd.read_csv(airport_file_loc, delimiter=\",\")\n",
    "\n",
    "# display a few lines of data to get to know it\n",
    "pd_airport.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# determine shape of the data, i.e. # columns and rows\n",
    "pd_airport.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get information about data\n",
    "pd_airport.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Since in the immigration data only airports with an international airport code are given (also known as IATA code), we will remove any rows without an IATA code from the airport table. Then, we will check how many rows are left in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# drop all rows without an IATA code\n",
    "pd_airport = pd_airport.dropna(subset=[\"iata_code\"])\n",
    "\n",
    "# see how the DataFrame has changed in size\n",
    "print(pd_airport.shape)\n",
    "pd_airport.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "For joining with the immigration table, columns like ident, local_code, gps_code, and continent are not as relevant, thus we will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# drop less relevant columns and verify\n",
    "pd_airport.drop(columns = [\"ident\", \"local_code\", \"continent\", \"gps_code\"], inplace=True)\n",
    "pd_airport.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "To make joins more intuitive, we will rename some of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# rename columns\n",
    "pd_airport.rename(columns = {\"type\": \"airport_type\",\n",
    "                            \"name\": \"airport_name\",\n",
    "                            \"iata_code\": \"airport_code\"},\n",
    "                 inplace = True)\n",
    "pd_airport.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# data quality check: relevant columns do not contain null/NaN\n",
    "if (pd_airport.isna().sum()[\"airport_name\"] == 0) and (pd_airport.isna().sum()[\"airport_code\"] == 0):\n",
    "    print(\"Data Quality Check - passed: No missing airport names or airport codes\")\n",
    "else:\n",
    "    print(\"Data Quality Check - FAILED: Missing airport names or airport codes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The last step is to write this data to files and copy these to S3, using Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\",\\\n",
    "                \"org.apache.hadoop:hadoop-aws:2.7.1,com.amazonaws:aws-java-sdk:1.7.4,saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# create schema for data\n",
    "schema = T.StructType([T.StructField(\"airport_type\", T.StringType()),\n",
    "                      T.StructField(\"airport_name\", T.StringType()),\n",
    "                      T.StructField(\"elevation_ft\", T.DoubleType()),\n",
    "                      T.StructField(\"iso_country\", T.StringType()),\n",
    "                      T.StructField(\"iso_region\", T.StringType()),\n",
    "                      T.StructField(\"municipality\", T.StringType()),\n",
    "                      T.StructField(\"airport_code\", T.StringType()),\n",
    "                      T.StructField(\"coordinates\", T.StringType())])\n",
    "\n",
    "# read data from pandas DataFrame into Spark DataFrame\n",
    "df_airport = spark.createDataFrame(pd_airport, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write data to parquet\n",
    "%time df_airport.write \\\n",
    "                .partitionBy('iso_country') \\\n",
    "                .mode('overwrite') \\\n",
    "                .parquet(os.path.join(local, airport_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# upload files to S3 using the AWS CLI\n",
    "# bash version: !aws s3 sync ./airport_codes/ s3://udacity-dend-capstone/airport_codes/ --only-show-errors\n",
    "aws_upload(airport_key, bucket)\n",
    "print(\"Airport data: uploaded to S3\")\n",
    "print(\"Airport data: PROCESSING FINISHED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Demographics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read data into pandas DataFrame\n",
    "pd_demographics = pd.read_csv(demographics_file_loc, delimiter=\";\")\n",
    "\n",
    "# look at a few rows to get to know the data\n",
    "pd_demographics.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# determine the size of the DataFrame\n",
    "pd_demographics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# look at the number of null/NaN values\n",
    "print(pd_demographics.isna().sum())\n",
    "\n",
    "# drop NaN values\n",
    "pd_demographics.dropna(inplace = True)\n",
    "\n",
    "# verify\n",
    "print(pd_demographics.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Race is not relevant in any way for Travely, so the colums Race and Count can be omitted. The information in the other columns is the same for any \"Race\", so no aggregations are necessary here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# drop irrelevant columns\n",
    "pd_demographics.drop(columns = [\"Race\", \"Count\"], inplace = True)\n",
    "\n",
    "# since now the rest of the rows is each a duplicate (or even a \"quadruplicate\"),\n",
    "# drop the repeated rows\n",
    "pd_demographics.drop_duplicates(inplace = True)\n",
    "\n",
    "# verify and look at size of DataFrame\n",
    "print(pd_demographics.shape)\n",
    "pd_demographics.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# rename columns for easier joining\n",
    "# and to match snake_case\n",
    "pd_demographics.rename(columns = {\"City\": \"city\",\n",
    "                                  \"State\": \"state\",\n",
    "                                  \"Median Age\": \"median_age\",\n",
    "                                  \"Male Population\": \"male_population\",\n",
    "                                  \"Female Population\": \"female_population\",\n",
    "                                  \"Total Population\": \"population\",\n",
    "                                  \"Number of Veterans\": \"veterans\",\n",
    "                                  \"Foreign-born\": \"foreign_born\",\n",
    "                                  \"Average Household Size\": \"avg_household_size\",\n",
    "                                  \"State Code\": \"state_code\"},\n",
    "                      inplace = True)\n",
    "pd_demographics.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check datatypes of columns\n",
    "pd_demographics.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# people occur only in integers -> change type to int64 for columns male_population, female_population, veterans, foreign_born\n",
    "pd_demographics = pd_demographics.astype({\"male_population\": \"int64\",\n",
    "                                          \"female_population\": \"int64\",\n",
    "                                          \"veterans\": \"int64\",\n",
    "                                          \"foreign_born\": \"int64\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# verify\n",
    "pd_demographics.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Data Quality Checks\n",
    "\n",
    "We would like the following conditions to be true:\n",
    "\n",
    "* population = male_population + female_population\n",
    "* population > foreign_born\n",
    "* population > avg_household_size\n",
    "* male_population > veterans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def data_quality_check(pd_dataframe, condition_wanted, message):\n",
    "    \"\"\"Check if condition_wanted is fulfilled in each of the dataframe's rows, \n",
    "    otherwise counts it as an error and then prints out the message with the total number of errors.\n",
    "    Arguments:\n",
    "    - pd_dataframe: a pandas dataframe\n",
    "    - condition_wanted: a condition every row should fulfill, as a string\n",
    "    - message: a message printed before the error count\"\"\"\n",
    "    errors = 0\n",
    "    for index, row in pd_dataframe.iterrows():\n",
    "        if not eval(condition_wanted):\n",
    "            errors += 1\n",
    "    print(message, errors)\n",
    "\n",
    "data_quality_check(pd_demographics,\n",
    "                   \"row.population >= (row.male_population + row.female_population)\",\n",
    "                   \"Data quality issues in population count: \")\n",
    "data_quality_check(pd_demographics,\n",
    "                   \"row.population > row.foreign_born\",\n",
    "                   \"Data quality issues in foreign_born: \")\n",
    "data_quality_check(pd_demographics,\n",
    "                   \"row.population > row.avg_household_size\",\n",
    "                   \"Data quality issues in avg_household_size: \")\n",
    "data_quality_check(pd_demographics,\n",
    "                   \"row.male_population > row.veterans\",\n",
    "                   \"Data quality issues in veterans: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\",\\\n",
    "                \"org.apache.hadoop:hadoop-aws:2.7.1,com.amazonaws:aws-java-sdk:1.7.4,saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read data from pandas DataFrame into Spark DataFrame\n",
    "df_demographics = spark.createDataFrame(pd_demographics)\n",
    "\n",
    "# write data to parquet locally\n",
    "%time df_demographics.write \\\n",
    "                .partitionBy('state_code') \\\n",
    "                .mode('overwrite') \\\n",
    "                .parquet(os.path.join(local, demographics_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# upload files to S3 using the AWS CLI\n",
    "# bash version: !aws s3 sync ./demographics/ s3://udacity-dend-capstone/demographics/ --only-show-errors\n",
    "aws_upload(demographics_key, bucket)\n",
    "print(\"Demographics data: uploaded to S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Demographics data: PROCESSING FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
