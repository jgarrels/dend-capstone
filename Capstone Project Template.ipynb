{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "## Data Engineering Capstone Project\n",
    "\n",
    "### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, monotonically_increasing_id, countDistinct\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, dayofweek\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read configuration file with AWS credentials\n",
    "config = configparser.ConfigParser()\n",
    "config.read('aws.cfg')\n",
    "\n",
    "# save as environment variables\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define the paths where the raw data is\n",
    "# Udacity workspace\n",
    "imm_folder_loc = \"../../data/18-83510-I94-Data-2016\"\n",
    "airport_file_loc = \"./airport-codes_csv.csv\"\n",
    "demographics_file_loc = \"./us-cities-demographics.csv\"\n",
    "temperature_file_loc = \"../../data2/GlobalLandTemperaturesByCity.csv\"\n",
    "\n",
    "# local version\n",
    "# imm_folder_loc = \"C:/Users/jlg/aws/dend-capstone-imm-data/data/18-83510-I94-Data-2016\"\n",
    "# airport_file_loc = \"./airport-codes_csv.csv\"\n",
    "# demographics_file_loc = \"./us-cities-demographics.csv\"\n",
    "# temperature_file_loc = \"C:/Users/jlg/aws/dend-capstone-temperature-data/GlobalLandTemperaturesByCity.csv\"\n",
    "\n",
    "# define the destination paths where the data should go, e.g. on S3\n",
    "# does this have any use with using the CLI? Should I use some kind of !bash command?\n",
    "dest_bucket = \"s3a://udacity-dend-capstone\"\n",
    "dest_imm_key = \"immigration/\"\n",
    "dest_airport_key = \"airport_codes/\"\n",
    "dest_demographics_key = \"demographics/\"\n",
    "dest_temperature_key = \"temperature/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "\n",
    "### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Scope of the project\n",
    "Travely, a US-based touristic tour provider, would like to analyze data of people traveling to the US. They would like to improve their offerings of day-tours and longer guided travels to meet their potential customers' needs. Additionally, they would like to know more about where and when people arrive so they can best advertise accordingly.\n",
    "\n",
    "For this, they would like to have a data lake including the most relevant data from the data sources listed below.\n",
    "\n",
    "For this project, I will use Spark to process the data, S3 to store it in parquet format and possibly a database service provided by AWS.\n",
    "\n",
    "### Describe and Gather Data\n",
    "I am using the datasets provided by Udacity. These include:\n",
    "\n",
    "**US immigration data**: A dataset that includes flight passenger data collected at immigration, such as the airport, arrival and departure, birthyear, gender, airline, etc.\n",
    "\n",
    "**Airport codes**: A dataset about airports, including their international and local codes, country, municipality, coordinates etc.\n",
    "\n",
    "**City temperature data**: A dataset about temperature in global cities, including data from the 18th to the 21st century, such as city, country, coordinates, temperature and temperature uncertainty.\n",
    "\n",
    "**US cities demographic**: A dataset about US cities, including the state, total population, and other factors such as average household size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 2: Explore and Assess the Data\n",
    "### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Immigration Data\n",
    "\n",
    "Here we work with Spark, which has a package supporting the SAS format (.sas7bdat). Spark is a bit slow, so when I ran the cells below, each one took up to 30 s.\n",
    "\n",
    "(In this case, pandas was a lot slower, so I chose Spark, although pandas also has an option to read in SAS files.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created for Immigration data\n",
      "Immigration data read into a Spark DataFrame\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .config(\"spark.jars.packages\",\\\n",
    "                \"org.apache.hadoop:hadoop-aws:2.7.1,com.amazonaws:aws-java-sdk:1.7.4,saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "print(\"SparkSession created for Immigration data\")\n",
    "\n",
    "df_imm =spark.read.format('com.github.saurfang.sas.spark')\\\n",
    "        .load(os.path.join(imm_folder_loc, 'i94_apr16_sub.sas7bdat'))\n",
    "print(\"Immigration data read into a Spark DataFrame\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.83 ms, sys: 4.23 ms, total: 8.06 ms\n",
      "Wall time: 5.33 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(cicid=6.0, i94yr=2016.0, i94mon=4.0, i94cit=692.0, i94res=692.0, i94port='XXX', arrdate=20573.0, i94mode=None, i94addr=None, depdate=None, i94bir=37.0, i94visa=2.0, count=1.0, dtadfile=None, visapost=None, occup=None, entdepa='T', entdepd=None, entdepu='U', matflag=None, biryear=1979.0, dtaddto='10282016', gender=None, insnum=None, airline=None, admnum=1897628485.0, fltno=None, visatype='B2'),\n",
       " Row(cicid=7.0, i94yr=2016.0, i94mon=4.0, i94cit=254.0, i94res=276.0, i94port='ATL', arrdate=20551.0, i94mode=1.0, i94addr='AL', depdate=None, i94bir=25.0, i94visa=3.0, count=1.0, dtadfile='20130811', visapost='SEO', occup=None, entdepa='G', entdepd=None, entdepu='Y', matflag=None, biryear=1991.0, dtaddto='D/S', gender='M', insnum=None, airline=None, admnum=3736796330.0, fltno='00296', visatype='F1'),\n",
       " Row(cicid=15.0, i94yr=2016.0, i94mon=4.0, i94cit=101.0, i94res=101.0, i94port='WAS', arrdate=20545.0, i94mode=1.0, i94addr='MI', depdate=20691.0, i94bir=55.0, i94visa=2.0, count=1.0, dtadfile='20160401', visapost=None, occup=None, entdepa='T', entdepd='O', entdepu=None, matflag='M', biryear=1961.0, dtaddto='09302016', gender='M', insnum=None, airline='OS', admnum=666643185.0, fltno='93', visatype='B2'),\n",
       " Row(cicid=16.0, i94yr=2016.0, i94mon=4.0, i94cit=101.0, i94res=101.0, i94port='NYC', arrdate=20545.0, i94mode=1.0, i94addr='MA', depdate=20567.0, i94bir=28.0, i94visa=2.0, count=1.0, dtadfile='20160401', visapost=None, occup=None, entdepa='O', entdepd='O', entdepu=None, matflag='M', biryear=1988.0, dtaddto='09302016', gender=None, insnum=None, airline='AA', admnum=92468461330.0, fltno='00199', visatype='B2'),\n",
       " Row(cicid=17.0, i94yr=2016.0, i94mon=4.0, i94cit=101.0, i94res=101.0, i94port='NYC', arrdate=20545.0, i94mode=1.0, i94addr='MA', depdate=20567.0, i94bir=4.0, i94visa=2.0, count=1.0, dtadfile='20160401', visapost=None, occup=None, entdepa='O', entdepd='O', entdepu=None, matflag='M', biryear=2012.0, dtaddto='09302016', gender=None, insnum=None, airline='AA', admnum=92468463130.0, fltno='00199', visatype='B2')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df_imm.head(5) # wall time 5-10s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jan loaded\n",
      "Immigration for feb loaded\n",
      "Immigration for feb appended\n",
      "Immigration for mar loaded\n",
      "Immigration for mar appended\n",
      "Immigration for apr loaded\n",
      "Immigration for apr appended\n",
      "Immigration for may loaded\n",
      "Immigration for may appended\n",
      "Immigration for jun loaded\n",
      "Immigration for jun appended\n",
      "Immigration for jul loaded\n",
      "Immigration for jul appended\n",
      "Immigration for aug loaded\n",
      "Immigration for aug appended\n",
      "Immigration for sep loaded\n",
      "Immigration for sep appended\n",
      "Immigration for oct loaded\n",
      "Immigration for oct appended\n",
      "Immigration for nov loaded\n",
      "Immigration for nov appended\n",
      "Immigration for dec loaded\n",
      "Immigration for dec appended\n",
      "Immigration: All months loaded\n"
     ]
    }
   ],
   "source": [
    "df_imm =spark.read.format('com.github.saurfang.sas.spark')\\\n",
    "        .load(os.path.join(imm_folder_loc, 'i94_jan16_sub.sas7bdat'))\n",
    "print(\"jan loaded\")\n",
    "months = [\"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]\n",
    "imm_columns = df_imm.columns\n",
    "\n",
    "# For some reason, the file for June has more columns, all starting with 'delete', probably deprecated.\n",
    "# I will thus select only the columns present in all of the dataframes.\n",
    "for month in months:\n",
    "    df_imm_next = spark.read.format('com.github.saurfang.sas.spark')\\\n",
    "                        .load(os.path.join(imm_folder_loc, f'i94_{month}16_sub.sas7bdat'))\n",
    "    print(f\"Immigration for {month} loaded\")\n",
    "    df_imm = df_imm.union(df_imm_next[imm_columns])\n",
    "    print(f\"Immigration for {month} appended\")\n",
    "\n",
    "print(\"Immigration: All months loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Let's see how many rows we have in this dataset (result: 40,790,529 rows). There are no duplicates (number of rows stays the same when dropping duplicates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# %time df_imm.count() # wall time 8min 50s - result: 40,790,529 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_imm = df_imm.dropDuplicates()\n",
    "# %time df_imm.count() # wall time 13min 38s - result: 40,790,529 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# select only people who came by plane -> i94mode==1\n",
    "df_imm = df_imm[df_imm['i94mode']==1]\n",
    "\n",
    "# %time df_imm.count() # -> reduced to 39,166,088\n",
    "# wall time 7min 58s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_imm = df_imm.dropna(subset=['i94yr', 'i94mon', 'i94port', 'arrdate', 'i94bir']) # wall time < 1s\n",
    "\n",
    "# %time df_imm.count() # wall time 7min 53s - result: 39,1458,783 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.55 ms, sys: 256 Âµs, total: 5.81 ms\n",
      "Wall time: 65.5 ms\n"
     ]
    }
   ],
   "source": [
    "# select the most relevant columns (explain why)\n",
    "%time df_imm = df_imm['i94yr', 'i94mon', 'i94port', 'i94addr', 'i94visa', 'arrdate', 'depdate', 'biryear', 'gender', 'visatype']\n",
    "# wall time < 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convert dates and add+compute duration column\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# rename columns, e.g. i94yr -> year, i94port -> airport_code\n",
    "# change this in the write task below as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write data to parquet files\n",
    "# Here I chose to write the parquet files locally and then use the aws cli to upload them to the S3 bucket.\n",
    "# Time comparison: 20 min writing + < 1min uploading \n",
    "# vs. writing directly to S3: not finished even after 6 hours\n",
    "\n",
    "%time df_imm.write \\\n",
    "            .partitionBy('i94mon').mode('overwrite') \\\n",
    "            .parquet(\"./imm_data/\")\n",
    "\n",
    "# maybe partition also by 'i94addr' ?\n",
    "# omit partition by i94yr for this dataset since it is only 2016, BUT INCLUDE THIS REASON IN THE WRITEUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "include details about cost and my choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The I94 immigration data also had some annotations in the file I94_SAS_Labels_Descriptions.SAS. From this, I have created the respective JSON files (such as i94addr_desc.json, in the desc folder), which will generate more tables for the final data model. (This is not included as code since I had to manually fix the files at some points.) Joining the immigration data and these tables will give more information about the encoded information, such as the state/country the immigrating person comes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# sync description json files to S3\n",
    "!aws s3 sync ./desc/ s3://udacity-dend-capstone/desc/ --exclude \"*ipynb*\" --only-show-errors\n",
    "print(\"Immigration description data: json files uploaded to S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# upload immigration parquet files to S3\n",
    "!aws s3 sync ./imm_data/ s3://udacity-dend-capstone/immigration/ --only-show-errors\n",
    "    # os.path.join(dest_bucket, dest_imm_key)\n",
    "print(\"Immigration data: parquet files uploaded to S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Immigration data: PROCESSING FINISHED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Temperature Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Since the raw data is in CSV format, I will use pandas for the exploration and cleaning because it is faster and provides an easy overview through its automatic table-formatting. Each step with pandas should take under 10 s.\n",
    "\n",
    "To write the data to parquet files in S3, I will use Pyspark - unlike pandas, it does not require additional dependencies for that task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%time pd_temperature = pd.read_csv(temperature_file_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(pd_temperature.shape)\n",
    "pd_temperature.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here we can already see there are some NaN values in the dataset. In the next step, we will determine how many values in which column are NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_temperature.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As we can see, only AverageTemperature and AverageTemperatureUncertainty have NaN values, and they have the same number of NaN values.\n",
    "Thus, we assume they are always either both NaN or both have a non-null value (as seen above in the first few lines of data).\n",
    "\n",
    "In the next step, we will drop the NaN values from the dataframe and verify there are no more NaN values in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_temperature = pd_temperature.dropna()\n",
    "print(pd_temperature.isnull().sum())\n",
    "pd_temperature.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Since Travely currently only operates in the US, we will select the temperature values for Country='United States', and also just include data after 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_temperature = pd_temperature[pd_temperature['Country']=='United States']\n",
    "pd_temperature = pd_temperature[pd_temperature['dt']>='2000-01-01']\n",
    "print(pd_temperature.shape)\n",
    "pd_temperature.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Before writing the table to file, I will rename the columns so they are a bit shorter (in the case of the temperature columns) and match the usual snake-case convention (e.g. AverageTemperature -> avg_temperature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_temperature.rename(columns = {\"AverageTemperature\": \"avg_temperature\",\n",
    "                                \"AverageTemperatureUncertainty\": \"avg_temperature_uncert\",\n",
    "                                \"City\": \"city\",\n",
    "                                \"Country\": \"country\",\n",
    "                                \"Latitude\": \"latitude\",\n",
    "                                \"Longitude\": \"longitude\"},\n",
    "                     inplace = True)\n",
    "pd_temperature.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Finally, we write the data into parquet files using Spark - after a small data quality check. The transformation of the pandas DataFrame to the Spark DataFrame, the file writing, and the S3 upload should each only take a few seconds.\n",
    "\n",
    "To avoid high charges for S3 LIST etc. operations and to make the process faster, I will first write the files to the workspace and then copy them to the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\",\\\n",
    "                \"org.apache.hadoop:hadoop-aws:2.7.1,com.amazonaws:aws-java-sdk:1.7.4,saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "df_temperature = spark.createDataFrame(pd_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check that dataframe is not empty\n",
    "if df_temperature.head(1) != 0:\n",
    "    print(\"Data Quality Check: data frame not empty, passed\")\n",
    "else:\n",
    "    print(\"DATAFRAME EMPTY\")\n",
    "\n",
    "# check that there are multiple cities and dates\n",
    "if df_temperature.groupby(\"city\").count().head(1) != 0 :\n",
    "    print(\"Data Quality Check: multiple cities, passed\")\n",
    "else:\n",
    "    print(\"Data Quality Check FAILED: missing cities\")\n",
    "\n",
    "if df_temperature.groupby(\"dt\").count().head(1) != 0:\n",
    "    print(\"Data Quality Check: multiple and dates, passed\")\n",
    "else:\n",
    "    print(\"Data Quality Check FAILED: missing or dates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%time df_temperature.write \\\n",
    "                .partitionBy('city') \\\n",
    "                .mode('overwrite') \\\n",
    "                .parquet(os.path.join(\".\", dest_temperature_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# upload files to S3 using the AWS CLI\n",
    "!aws s3 sync ./temperature/ s3://udacity-dend-capstone/temperature/ --only-show-errors\n",
    "print(\"Temperature data: uploaded to S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Temperature data: PROCESSING FINISHED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Airport Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_airport = pd.read_csv(airport_file_loc, delimiter=\",\")\n",
    "pd_airport.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_airport.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_airport.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Since in the immigration data only airports with an international airport code are given (also known as IATA code), we will remove any rows without an IATA code from the airport table. Then, we will check how many rows are left in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_airport = pd_airport.dropna(subset=[\"iata_code\"])\n",
    "pd_airport.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "For joining with the immigration table, columns like ident, local_code, gps_code, and continent are not as relevant, thus we will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_airport.drop(columns = [\"ident\", \"local_code\", \"continent\", \"gps_code\"], inplace=True)\n",
    "pd_airport.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Having the coordinates together in a tuple is not very convenient. Two double-type columns latitude and longitude seem more beneficial for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#pd_airport['coordinates'].map(eval()).head(5)\n",
    "#pd_airport.head(5)\n",
    "type(pd_airport.coordinates[1088])\n",
    "#pd_airport['latitude'] = pd_airport['coordinates'][0]\n",
    "pd_airport[['latitude', 'longitude']] = pd.DataFrame(pd_airport['coordinates'].tolist(), index=pd_airport.index)\n",
    "pd_airport.drop(columns=['coordinates'], inplace=True)\n",
    "pd_airport.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "To make joins more intuitive, we will rename some of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_airport.rename(columns = {\"type\": \"airport_type\",\n",
    "                            \"name\": \"airport_name\",\n",
    "                            \"iata_code\": \"airport_code\"},\n",
    "                 inplace = True)\n",
    "pd_airport.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# data quality check: relevant columns do not contain null/NaN\n",
    "if (pd_airport.isna().sum()[\"airport_name\"] == 0) and (pd_airport.isna().sum()[\"airport_code\"] == 0):\n",
    "    print(\"Data Quality Check - passed: No missing airport names or airport codes\")\n",
    "else:\n",
    "    print(\"Data Quality Check - FAILED: Missing airport names or airport codes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The last step is to write this data to files and copy these to S3, using Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\",\\\n",
    "                \"org.apache.hadoop:hadoop-aws:2.7.1,com.amazonaws:aws-java-sdk:1.7.4,saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "schema = T.StructType([T.StructField(\"airport_type\", T.StringType()),\n",
    "                      T.StructField(\"airport_name\", T.StringType()),\n",
    "                      T.StructField(\"elevation_ft\", T.DoubleType()),\n",
    "                      T.StructField(\"iso_country\", T.StringType()),\n",
    "                      T.StructField(\"iso_region\", T.StringType()),\n",
    "                      T.StructField(\"municipality\", T.StringType()),\n",
    "                      T.StructField(\"airport_code\", T.StringType()),\n",
    "                      T.StructField(\"latitude\", T.DoubleType()),\n",
    "                      T.StructField(\"longitude\", T.DoubleType())])\n",
    "\n",
    "df_airport = spark.createDataFrame(pd_airport, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%time df_airport.write \\\n",
    "                .partitionBy('iso_country') \\\n",
    "                .mode('overwrite') \\\n",
    "                .parquet(os.path.join(\".\", dest_airport_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# upload files to S3 using the AWS CLI\n",
    "!aws s3 sync ./airport_codes/ s3://udacity-dend-capstone/airport_codes/ --only-show-errors\n",
    "print(\"Airport data: uploaded to S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Demographics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_demographics = pd.read_csv(demographics_file_loc, delimiter=\";\")\n",
    "pd_demographics.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_demographics[pd_demographics['City'] == \"Silver Spring\"].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_demographics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(pd_demographics.isna().sum())\n",
    "pd_demographics.dropna(inplace = True)\n",
    "print(pd_demographics.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Race is not relevant in any way for Travely, so the colums Race and Count can be omitted. The information in the other columns is the same for any \"Race\", so no aggregations are necessary here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_demographics.drop(columns = [\"Race\", \"Count\"], inplace = True)\n",
    "pd_demographics.drop_duplicates(inplace = True)\n",
    "print(pd_demographics.shape)\n",
    "pd_demographics.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_demographics.rename(columns = {\"City\": \"city\",\n",
    "                                  \"State\": \"state\",\n",
    "                                  \"Median Age\": \"median_age\",\n",
    "                                  \"Male Population\": \"male_population\",\n",
    "                                  \"Female Population\": \"female_population\",\n",
    "                                  \"Total Population\": \"population\",\n",
    "                                  \"Number of Veterans\": \"veterans\",\n",
    "                                  \"Foreign-born\": \"foreign_born\",\n",
    "                                  \"Average Household Size\": \"avg_household_size\",\n",
    "                                  \"State Code\": \"state_code\"},\n",
    "                      inplace = True)\n",
    "pd_demographics.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_demographics.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# people occur only in integers -> change type to int64 for columns male_population, female_population, veterans, foreign_born\n",
    "pd_demographics = pd_demographics.astype({\"male_population\": \"int64\",\n",
    "                                          \"female_population\": \"int64\",\n",
    "                                          \"veterans\": \"int64\",\n",
    "                                          \"foreign_born\": \"int64\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_demographics.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Data Quality Checks\n",
    "\n",
    "We would like the following conditions to be true:\n",
    "\n",
    "population = male_population + female_population\n",
    "population > foreign_born\n",
    "population > avg_household_size\n",
    "male_population > veterans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "errors = 0\n",
    "\n",
    "for index, row in pd_demographics.iterrows():\n",
    "    if (row.population < (row.male_population + row.female_population)):\n",
    "        # since some people might self-identify as neither male or female, there could be more, but not less people than males+females\n",
    "        errors += 1\n",
    "print(f\"Data quality issues in population count: {errors}\")\n",
    "\n",
    "errors = 0\n",
    "for index, row in pd_demographics.iterrows():\n",
    "    if row.population < row.foreign_born:\n",
    "        errors += 1\n",
    "print(f\"Data quality issues in foreign_born: {errors}\")\n",
    "errors = 0\n",
    "\n",
    "for index, row in pd_demographics.iterrows():\n",
    "    if row.population < row.avg_household_size:\n",
    "        errors += 1\n",
    "print(f\"Data quality issues in avg_household_size: {errors}\")\n",
    "errors = 0\n",
    "\n",
    "for index, row in pd_demographics.iterrows():\n",
    "    if row.male_population < row.veterans:\n",
    "        errors += 1\n",
    "print(f\"Data quality issues in veterans: {errors}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\",\\\n",
    "                \"org.apache.hadoop:hadoop-aws:2.7.1,com.amazonaws:aws-java-sdk:1.7.4,saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demographics = spark.createDataFrame(pd_demographics)\n",
    "\n",
    "%time df_demographics.write \\\n",
    "                .partitionBy('state_code') \\\n",
    "                .mode('overwrite') \\\n",
    "                .parquet(os.path.join(\".\", dest_demographics_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# upload files to S3 using the AWS CLI\n",
    "!aws s3 sync ./demographics/ s3://udacity-dend-capstone/demographics/ --only-show-errors\n",
    "print(\"Demographics data: uploaded to S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
